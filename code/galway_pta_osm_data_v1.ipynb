{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Galway Public Transport Accessibility Score Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain openai pandas tabulate\n",
    "# # ! pip install langchain-community\n",
    "# ! pip install langchain-ollama\n",
    "# !pip install thefuzz \n",
    "# !pip install python-Levenshtein\n",
    "# !pip install osmnx\n",
    "\n",
    "# ! pip install geopandas \n",
    "# !pip install selenium\n",
    "\n",
    "# !pip uninstall numpy -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/njindal/Documents/aic2025/data'\n",
    "code_dir = '/Users/njindal/Documents/aic2025/code'\n",
    "artifact_dir = '/Users/njindal/Documents/aic2025/artifacts'\n",
    "\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) # display full column width\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import shutil # For removing the directory if needed for a clean re-extract\n",
    "from selenium import webdriver # Used for scraping bus timetables from buseireann.ie\n",
    "import time\n",
    "# !pip install thefuzz \n",
    "# !pip install python-Levenshtein\n",
    "import re # For regular expressions\n",
    "from thefuzz import process, fuzz\n",
    "import networkx as nx\n",
    "\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import osmnx as ox\n",
    "import matplotlib\n",
    "import matplotlib.patheffects as patheffects\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import warnings\n",
    "import matplotlib.patheffects as path_effects\n",
    "import pickle\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# References for Data \n",
    "# https://libguides.ucd.ie/gisguide/findspatialdata \n",
    "# https://download.geofabrik.de/europe/ireland-and-northern-ireland.html\n",
    "# https://galway-bus.apis.ie/gstoptimes/#g-stop-time-schema\n",
    "# https://tilburgsciencehub.com/topics/visualization/data-visualization/graphs-charts/grammar-of-graphics-ggplot2/\n",
    "# https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "# https://react-lm.github.io/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook, we perform:**\n",
    "\n",
    "- Data Collection, Validation, Cleaning & Transformation\n",
    "\n",
    "- Exploratory Data Analysis (EDA) & POI Definition: Perform EDA and define Points of Interest (POIs)\n",
    "\n",
    "- Network Graph Construction & Public Transport Mapping: Combine graph construction with the mapping of POIs and bus stops\n",
    "\n",
    "- Accessibility Metric Definition, Score Calculation & Analysis \n",
    "\n",
    "- Visualization of POIs on Galway Map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Galway Bus API - Bus Stops\n",
    "\n",
    "Reference: https://galway-bus.apis.ie\n",
    "\n",
    "Attributes: stop_id, stop_name, stop_lat, stop_lon, direction, and route_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the CSV file\n",
    "data_dir = '/Users/njindal/Documents/aic2025/data' \n",
    "output_filename = 'gstops_df_v1.csv'\n",
    "output_path = os.path.join(data_dir, output_filename)\n",
    "\n",
    "# Try to load the DataFrame from CSV \n",
    "if os.path.exists(output_path):\n",
    "    print(f\"Loading existing gstops_df_v1 from: {output_path}\")\n",
    "    # Read the CSV and set the first column as the index\n",
    "    gstops_df_v1 = pd.read_csv(output_path, index_col=0)\n",
    "    print(f\"Loaded DataFrame with shape: {gstops_df_v1.shape}\")\n",
    "else:\n",
    "    print(f\"File not found at {output_path}. Fetching data from API...\")\n",
    "    route_ids = [401, 402, 404, 405, 407, 409, 410, 411, 412, 414]\n",
    "    all_stops = []\n",
    "\n",
    "    for route_id in route_ids:\n",
    "        url = f\"https://galway-bus.apis.ie/api/groute/{route_id}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                results = data.get('results', [])\n",
    "                if isinstance(results, dict): # results may be a dict or a list\n",
    "                    results = [results]\n",
    "                if not results:\n",
    "                    print(f\"Route id {route_id} information not available from API.\")\n",
    "                    continue\n",
    "                for direction_info in results:\n",
    "                    direction_id = direction_info.get('direction_id')\n",
    "                    for stop in direction_info.get('g_stops', []):\n",
    "                        stop_row = {\n",
    "                            'stop_id': stop['stop_id'],\n",
    "                            'stop_name': stop['stop_name'],\n",
    "                            'stop_lat': stop['stop_lat'],\n",
    "                            'stop_lon': stop['stop_lon'],\n",
    "                            'direction': direction_id,\n",
    "                            'route_id': route_id\n",
    "                        }\n",
    "                        all_stops.append(stop_row)\n",
    "            else:\n",
    "                print(f\"Route id {route_id} information not available from API (HTTP {response.status_code}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for route id {route_id} from API: {e}\")\n",
    "\n",
    "    if all_stops: # to check if any stops were collected\n",
    "        gstops_df_v1_from_api = pd.DataFrame(all_stops) # create a temporary DataFrame\n",
    "\n",
    "        # --- Create custom 'BS' indices ---\n",
    "        print(\"\\nCreating custom 'BS' indices for new API data...\")\n",
    "        if all(col in gstops_df_v1_from_api.columns for col in ['route_id', 'direction', 'stop_id']):\n",
    "            gstops_df_v1_sorted = gstops_df_v1_from_api.sort_values(by=['route_id', 'direction', 'stop_id']).reset_index(drop=True)\n",
    "        else:\n",
    "            print(\"Warning: Columns for sorting ('route_id', 'direction', 'stop_id') not all present. Indexing based on current order.\")\n",
    "            gstops_df_v1_sorted = gstops_df_v1_from_api.reset_index(drop=True)\n",
    "\n",
    "        bus_stop_indices = [f'BS{i+1}' for i in range(len(gstops_df_v1_sorted))]\n",
    "        gstops_df_v1_sorted.index = bus_stop_indices\n",
    "        gstops_df_v1 = gstops_df_v1_sorted \n",
    "        # --- End of custom 'BS' indices creation ---\n",
    "\n",
    "        # save the DataFrame with the index\n",
    "        gstops_df_v1.to_csv(output_path, index=True)\n",
    "        print(f\"gstops_df_v1 fetched from API, custom 'BS' indices created, and saved to {output_path}.\")\n",
    "        print(f\"Shape of new gstops_df_v1: {gstops_df_v1.shape}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No stop data collected from API. gstops_df_v1 is empty.\")\n",
    "        # Create an empty DataFrame with expected columns if no data was fetched\n",
    "        gstops_df_v1 = pd.DataFrame(columns=['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'direction', 'route_id'])\n",
    "        gstops_df_v1.index.name = 'bs_index' \n",
    "\n",
    "\n",
    "print(\"\\n--- gstops_df_v1 Final State ---\")\n",
    "if not gstops_df_v1.empty:\n",
    "    print(gstops_df_v1.head())\n",
    "else:\n",
    "    print(\"gstops_df_v1 is empty.\")\n",
    "print(f\"Shape of final gstops_df_v1: {gstops_df_v1.shape}\")\n",
    "print(f\"Index of final gstops_df_v1: {gstops_df_v1.index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bus Eireann routes-and-timetables\n",
    "\n",
    "Reference: https://www.buseireann.ie/routes-and-timetables/401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_ids = [401, 402, 404, 405, 407, 409, 410, 411, 412, 414]\n",
    "all_timetables_df = pd.DataFrame()\n",
    "output_csv_filename = 'all_bus_eirean_timetables.csv'\n",
    "output_csv_path = os.path.join(data_dir, output_csv_filename) \n",
    "\n",
    "# 1. Determining loading/scraping path\n",
    "if os.path.exists(output_csv_path):\n",
    "    print(f\"Loading existing timetable data from: {output_csv_path}\")\n",
    "    all_timetables_df = pd.read_csv(output_csv_path)\n",
    "    print(f\"Loaded DataFrame with shape: {all_timetables_df.shape}\")\n",
    "else:\n",
    "    print(f\"File not found at {output_csv_path}. Proceeding with web scraping...\")\n",
    "    driver = None # Initialize driver\n",
    "    try:\n",
    "        print(\"Attempting to set up Chrome WebDriver...\")\n",
    "        driver = webdriver.Chrome() # setup if chromedriver is in PATH\n",
    "        print(\"WebDriver setup successful.\")\n",
    "\n",
    "        for route_id in route_ids:\n",
    "            url = f\"https://www.buseireann.ie/routes-and-timetables/{route_id}\"\n",
    "            print(f\"Processing route: {route_id} from URL: {url}\")\n",
    "            driver.get(url)\n",
    "            print(f\"Opened URL: {url}\")\n",
    "\n",
    "            time.sleep(5) \n",
    "\n",
    "            print(f\"Attempting to get current table for route {route_id}...\")\n",
    "            page_html = driver.page_source\n",
    "\n",
    "            try:\n",
    "                list_of_dataframes = pd.read_html(page_html)\n",
    "                if list_of_dataframes:\n",
    "                    print(f\"Found {len(list_of_dataframes)} table(s) for route {route_id}.\")\n",
    "                    timetable_df = list_of_dataframes[0]  \n",
    "                    timetable_df['route_id'] = route_id\n",
    "                    print(f\"Extracted DataFrame for route {route_id}:\")\n",
    "                    all_timetables_df = pd.concat([all_timetables_df, timetable_df], ignore_index=True)\n",
    "                else:\n",
    "                    print(f\"No tables found for route {route_id} on page: {url}\")\n",
    "            except ValueError as ve:\n",
    "                print(f\"No tables found by pandas for route {route_id} (pd.read_html error: {ve})\")\n",
    "            except Exception as table_ex:\n",
    "                print(f\"Could not parse tables for route {route_id}: {table_ex}\")\n",
    "        \n",
    "        # Save the scraped data\n",
    "        if not all_timetables_df.empty:\n",
    "            all_timetables_df.columns = [col.replace('.', '_') for col in all_timetables_df.columns]\n",
    "            all_timetables_df.to_csv(output_csv_path, index=False)\n",
    "            print(f\"Combined timetable scraped and saved to {output_csv_path}\")\n",
    "        else:\n",
    "            print(\"No timetable data was extracted during scraping.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Selenium operations: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing WebDriver.\")\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "print(\"\\n--- Processed Timetable Data ---\")\n",
    "if not all_timetables_df.empty:\n",
    "    print(f\"Total rows in final DataFrame: {len(all_timetables_df)}\")\n",
    "    print(\"\\nUnique route_ids in DataFrame:\", all_timetables_df['route_id'].unique())\n",
    "else:\n",
    "    print(\"No timetable data available.\")\n",
    "\n",
    "\n",
    "if not all_timetables_df.empty:\n",
    "    required_columns = ['route_id', 'ROUTE'] \n",
    "    \n",
    "    # Check if these columns exist\n",
    "    available_columns = [col for col in required_columns if col in all_timetables_df.columns]\n",
    "    \n",
    "    if len(available_columns) == len(required_columns):\n",
    "        bus_timetables = all_timetables_df[available_columns].copy() # use .copy() to avoid SettingWithCopyWarning error\n",
    "        print(\"\\n--- bus_timetables DataFrame ---\")\n",
    "        print(f\"Shape of bus_timetables: {bus_timetables.shape}\")\n",
    "    else:\n",
    "        print(f\"\\nError: Not all required columns ({required_columns}) found in all_timetables_df.\")\n",
    "        print(f\"Available columns: {list(all_timetables_df.columns)}\")\n",
    "        bus_timetables = pd.DataFrame() # Create an empty DataFrame\n",
    "else:\n",
    "    print(\"\\nall_timetables_df is empty. Cannot create bus_timetables.\")\n",
    "    bus_timetables = pd.DataFrame() # Create an empty DataFrame\n",
    "\n",
    "if 'bus_timetables' in locals() and not bus_timetables.empty:\n",
    "    print(\"\\nAdding 'stop_order_on_route' column (assuming pre-sorted data within each route_id)...\")\n",
    "    \n",
    "    if not bus_timetables.groupby('route_id').ngroup().is_monotonic_increasing:\n",
    "         print(\"Sorting by 'route_id' to ensure contiguous groups for cumcount...\")\n",
    "         bus_timetables = bus_timetables.sort_values(by='route_id', kind='mergesort').reset_index(drop=True)\n",
    "    \n",
    "    bus_timetables['stop_order_on_route'] = bus_timetables.groupby('route_id').cumcount()\n",
    "    \n",
    "    print(\"'stop_order_on_route' column added.\")\n",
    "    print(\"\\n--- bus_timetables DataFrame with stop_order_on_route ---\")\n",
    "    print(f\"Shape of bus_timetables: {bus_timetables.shape}\")\n",
    "    \n",
    "\n",
    "    unique_routes_to_sample = bus_timetables['route_id'].unique()\n",
    "    if len(unique_routes_to_sample) > 0:\n",
    "        sample_route_id = unique_routes_to_sample[0]\n",
    "        print(f\"\\nSample for route_id '{sample_route_id}':\")\n",
    "        print(bus_timetables[bus_timetables['route_id'] == sample_route_id][['ROUTE', 'route_id', 'stop_order_on_route']].head(10))\n",
    "        if len(bus_timetables[bus_timetables['route_id'] == sample_route_id]) > 10:\n",
    "            print(\"...\")\n",
    "            print(bus_timetables[bus_timetables['route_id'] == sample_route_id][['ROUTE', 'route_id', 'stop_order_on_route']].tail(5))\n",
    "\n",
    "    if len(unique_routes_to_sample) > 1:\n",
    "        sample_route_id_2 = unique_routes_to_sample[1]\n",
    "        print(f\"\\nSample for route_id '{sample_route_id_2}':\")\n",
    "        print(bus_timetables[bus_timetables['route_id'] == sample_route_id_2][['ROUTE', 'route_id', 'stop_order_on_route']].head(10))\n",
    "\n",
    "else:\n",
    "    print(\"\\nbus_timetables DataFrame is not defined or is empty. Cannot add 'stop_order_on_route'.\")\n",
    "\n",
    "display(bus_timetables[bus_timetables['route_id'] == 401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Galway Bus API - Bus Routes\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "- route_long_name: Full name of the route\n",
    "\n",
    "- g_trip_headsign: Destination displayed on the bus\n",
    "\n",
    "- route_id: Unique identifier for the route\n",
    "\n",
    "- route_short_name: Short route number (e.g. 401, 402)\n",
    "\n",
    "- direction_id: Direction of travel (0 or 1)\n",
    "\n",
    "- first_stop_id: ID of the first stop\n",
    "\n",
    "- last_stop_id: ID of the last stop\n",
    "\n",
    "- first_stop_name: Name of the first stop\n",
    "\n",
    "- last_stop_name: Name of the last stop\n",
    "\n",
    "- num_stops: Total number of stops on the route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the CSV file for gvariations_df_v1\n",
    "gvariations_output_filename = 'gvariations_df_v1.csv'\n",
    "gvariations_output_path = os.path.join(data_dir, gvariations_output_filename) # Ensure data_dir is defined\n",
    "\n",
    "# Try to load the DataFrame from CSV first\n",
    "if os.path.exists(gvariations_output_path):\n",
    "    print(f\"Loading existing gvariations_df_v1 from: {gvariations_output_path}\")\n",
    "    # Read the CSV and set the first column as the index\n",
    "    gvariations_df_v1 = pd.read_csv(gvariations_output_path, index_col=0)\n",
    "    print(f\"Loaded gvariations_df_v1 DataFrame with shape: {gvariations_df_v1.shape}\")\n",
    "else:\n",
    "    print(f\"File not found at {gvariations_output_path}. Fetching gvariations data from API...\")\n",
    "    route_ids = [401, 402, 404, 405, 407, 409, 410, 411, 412, 414]\n",
    "    all_variations = []\n",
    "\n",
    "    for route_id in route_ids:\n",
    "        url = f\"https://galway-bus.apis.ie/api/groute/{route_id}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                results = data.get('results', [])\n",
    "                if isinstance(results, dict): # results may be a dict or a list\n",
    "                    results = [results]\n",
    "                if not results:\n",
    "                    print(f\"Route id {route_id} (variations) - information not available from API.\")\n",
    "                    continue\n",
    "                for direction_info in results:\n",
    "                    route_long_name = direction_info.get('route_long_name')\n",
    "                    g_trip_headsign = direction_info.get('g_trip_headsign')\n",
    "                    route_short_name = direction_info.get('route_short_name')\n",
    "                    direction_id = direction_info.get('direction_id')\n",
    "                    for variation in direction_info.get('g_route_variations', []):\n",
    "                        row = {\n",
    "                            'route_long_name': route_long_name,\n",
    "                            'g_trip_headsign': g_trip_headsign,\n",
    "                            'route_id': route_id, # Queried route_id\n",
    "                            'route_short_name': route_short_name,\n",
    "                            'direction_id': direction_id,\n",
    "                            'variation_route_id': variation.get('route_id'), # ID from g_route_variations\n",
    "                            'first_stop_id': variation.get('first_stop_id'),\n",
    "                            'last_stop_id': variation.get('last_stop_id'),\n",
    "                            'first_stop_name': variation.get('first_stop_name'),\n",
    "                            'last_stop_name': variation.get('last_stop_name'),\n",
    "                            'num_stops': variation.get('num_stops')\n",
    "                        }\n",
    "                        all_variations.append(row)\n",
    "            else:\n",
    "                print(f\"Route id {route_id} (variations) - information not available from API (HTTP {response.status_code}).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching variations data for route id {route_id} from API: {e}\")\n",
    "\n",
    "    if all_variations: # Check if any variations were collected\n",
    "        gvariations_df_from_api = pd.DataFrame(all_variations)\n",
    "\n",
    "        # --- Create custom 'BR' indices ---\n",
    "        print(\"\\nCreating custom 'BR' indices for new API gvariations_data...\")\n",
    "        # Define sorting columns \n",
    "        sort_columns = ['route_id', 'direction_id', 'variation_route_id', 'first_stop_id']\n",
    "        if all(col in gvariations_df_from_api.columns for col in sort_columns):\n",
    "            gvariations_df_sorted = gvariations_df_from_api.sort_values(by=sort_columns).reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"Warning: Not all columns for sorting ({sort_columns}) present in gvariations. Indexing based on current order.\")\n",
    "            gvariations_df_sorted = gvariations_df_from_api.reset_index(drop=True)\n",
    "\n",
    "        bus_route_variation_indices = [f'BR{i+1}' for i in range(len(gvariations_df_sorted))]\n",
    "        gvariations_df_sorted.index = bus_route_variation_indices\n",
    "        gvariations_df_v1 = gvariations_df_sorted \n",
    "        # --- End of custom 'BR' indices creation ---\n",
    "\n",
    "\n",
    "        # Save the DataFrame WITH THE INDEX\n",
    "        gvariations_df_v1.to_csv(gvariations_output_path, index=True)\n",
    "        print(f\"gvariations_df_v1 fetched from API, custom 'BR' indices created, and saved to {gvariations_output_path}.\")\n",
    "        print(f\"Shape of new gvariations_df_v1: {gvariations_df_v1.shape}\")\n",
    "    else:\n",
    "        print(\"No route variation data collected from API. gvariations_df_v1 is empty.\")\n",
    "        gvariations_df_v1 = pd.DataFrame(columns=[\n",
    "            'route_long_name', 'g_trip_headsign', 'route_id', 'route_short_name',\n",
    "            'direction_id', 'variation_route_id', 'first_stop_id', 'last_stop_id',\n",
    "            'first_stop_name', 'last_stop_name', 'num_stops'\n",
    "        ])\n",
    "        gvariations_df_v1.index.name = 'br_index' \n",
    "\n",
    "\n",
    "print(\"\\n--- gvariations_df_v1 Final State ---\")\n",
    "if not gvariations_df_v1.empty:\n",
    "    display(gvariations_df_v1.head())\n",
    "else:\n",
    "    print(\"gvariations_df_v1 is empty.\")\n",
    "print(f\"Shape of final gvariations_df_v1: {gvariations_df_v1.shape}\")\n",
    "print(f\"Index of final gvariations_df_v1: {gvariations_df_v1.index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download the Ireland and Northern Ireland Shape Files\n",
    "\n",
    "https://libguides.ucd.ie/gisguide/findspatialdata  \n",
    "\n",
    "https://download.geofabrik.de/europe/ireland-and-northern-ireland.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for Geofabrik Download ---\n",
    "geofabrik_url = \"https://download.geofabrik.de/europe/ireland-and-northern-ireland-latest-free.shp.zip\"\n",
    "download_target_dir = data_dir # Or your desired common data directory\n",
    "zip_filename = os.path.join(download_target_dir, \"ireland-and-northern-ireland-latest-free.shp.zip\")\n",
    "extracted_shapefile_dir = os.path.join(download_target_dir, \"ireland-and-northern-ireland-latest-free.shp\")\n",
    "\n",
    "\n",
    "# --- Download and Unzip Logic ---\n",
    "def download_and_extract_osm_data(url, zip_path, extract_to_path):\n",
    "    \"\"\"Downloads and extracts OSM shapefile data if not already present.\"\"\"\n",
    "    try:\n",
    "        # Check if the final extracted directory already exists and has files (e.g., roads.shp)\n",
    "        expected_roads_shp = os.path.join(extract_to_path, 'gis_osm_roads_free_1.shp')\n",
    "        if os.path.exists(expected_roads_shp):\n",
    "            print(f\"Shapefile data already found at: {extract_to_path}\")\n",
    "            return True\n",
    "\n",
    "        # If not fully extracted, or zip file is missing, proceed to download\n",
    "        if not os.path.exists(zip_path):\n",
    "            print(f\"Downloading OSM data from {url} to {zip_path}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status() # Will raise an HTTP error if the HTTP request returned an unsuccessful status code\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(\"Download complete.\")\n",
    "        else:\n",
    "            print(f\"Zip file already exists at {zip_path}. Proceeding to extraction.\")\n",
    "\n",
    "\n",
    "        print(f\"Extracting {zip_path} to {extract_to_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to_path)\n",
    "        print(\"Extraction complete.\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: Downloaded file at {zip_path} is not a valid zip file or is corrupted.\")\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during download/extraction: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if download_and_extract_osm_data(geofabrik_url, zip_filename, extracted_shapefile_dir):\n",
    "    print(\"OSM data is ready.\")\n",
    "    shapefile_base_dir = extracted_shapefile_dir\n",
    "    print(f\"Shapefiles will be loaded from: {shapefile_base_dir}\")\n",
    "else:\n",
    "    print(\"Failed to prepare OSM data. Please check the errors. Exiting or using fallback.\")\n",
    "    shapefile_base_dir = extracted_shapefile_dir \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Rahoon-Portershed Public Transport Accessibility Map for Galway - [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure osmnx settings and logging\n",
    "# ox.config(log_console=True, use_cache=False)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "place_name = \"Galway, Ireland\"\n",
    "shapefile_base_dir = '/Users/njindal/Documents/aic2025/data/ireland-and-northern-ireland-latest-free.shp' \n",
    "\n",
    "shapefile_layers = {\n",
    "    'roads': 'gis_osm_roads_free_1.shp',\n",
    "    'water_poly': 'gis_osm_water_a_free_1.shp',\n",
    "    'railways': 'gis_osm_railways_free_1.shp',\n",
    "    'waterways': 'gis_osm_waterways_free_1.shp',\n",
    "    'landuse': 'gis_osm_landuse_a_free_1.shp',\n",
    "    'buildings': 'gis_osm_buildings_a_free_1.shp',\n",
    "    'places_poly': 'gis_osm_places_a_free_1.shp'\n",
    "}\n",
    "\n",
    "print(f\"\\n--- Processing Data for: {place_name} ---\")\n",
    "print(f\"Using Shapefile directory: {shapefile_base_dir}\")\n",
    "\n",
    "try:\n",
    "    # --- *** GET GALWAY BOUNDARY *** ---\n",
    "    print(\"\\nFetching boundary for Galway...\")\n",
    "    boundary_gdf = ox.geocode_to_gdf(place_name).to_crs(\"EPSG:4326\")\n",
    "    if boundary_gdf.empty:\n",
    "        raise ValueError(f\"Could not geocode '{place_name}'.\")\n",
    "    print(f\"Boundary fetched. CRS set to: {boundary_gdf.crs}\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- *** LOAD IRELAND SHAPEFILES & CLIP TO GALWAY BOUNDARY *** ---\n",
    "    print(\"\\nLoading and clipping Ireland-wide layers to Galway boundary...\")\n",
    "    galway_gdfs = {}\n",
    "    for layer_name, shp_filename in shapefile_layers.items():\n",
    "        shp_path = os.path.join(shapefile_base_dir, shp_filename)\n",
    "        print(f\"--- Processing layer: {layer_name} ---\")\n",
    "        if not os.path.exists(shp_path):\n",
    "            print(f\"*** WARNING: Shapefile not found: {shp_path} - Skipping layer '{layer_name}' ***\")\n",
    "            continue\n",
    "        try:\n",
    "            ireland_layer_gdf = gpd.read_file(shp_path)\n",
    "            if ireland_layer_gdf.crs != boundary_gdf.crs:\n",
    "                ireland_layer_gdf = ireland_layer_gdf.to_crs(boundary_gdf.crs)\n",
    "            clipped_gdf = gpd.clip(ireland_layer_gdf, boundary_gdf, keep_geom_type=True)\n",
    "            if not clipped_gdf.empty:\n",
    "                galway_gdfs[layer_name] = clipped_gdf\n",
    "            else:\n",
    "                print(f\"Note: No features found for layer '{layer_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"*** ERROR processing layer '{layer_name}': {e} ***\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- *** PREPARE BUS STOP GEODATAFRAME FROM GSTOPS_DF_V1 *** --- \n",
    "    print(\"\\nPreparing Galway Bus Stop data from gstops_df_v1...\")\n",
    "    bus_stops_gdf = None\n",
    "    if 'gstops_df_v1' in locals() and isinstance(gstops_df_v1, pd.DataFrame) and not gstops_df_v1.empty:\n",
    "        # Ensure 'stop_lat' and 'stop_lon' columns exist\n",
    "        if 'stop_lat' in gstops_df_v1.columns and 'stop_lon' in gstops_df_v1.columns:\n",
    "            try:\n",
    "                # Drop rows with invalid (NaN) coordinates before creating GeoDataFrame\n",
    "                temp_stops_df = gstops_df_v1.dropna(subset=['stop_lat', 'stop_lon']).copy()\n",
    "                \n",
    "                if not temp_stops_df.empty:\n",
    "                    bus_stops_gdf = gpd.GeoDataFrame(\n",
    "                        temp_stops_df,\n",
    "                        geometry=gpd.points_from_xy(temp_stops_df['stop_lon'], temp_stops_df['stop_lat']),\n",
    "                        crs=\"EPSG:4326\"  \n",
    "                    )\n",
    "                    print(f\"Created GeoDataFrame 'bus_stops_gdf' with {len(bus_stops_gdf)} stops from gstops_df_v1.\")\n",
    "                    # Reproject if CRS doesn't match the boundary CRS\n",
    "                    if bus_stops_gdf.crs != boundary_gdf.crs:\n",
    "                        print(f\"Reprojecting bus stops GDF to {boundary_gdf.crs}...\");\n",
    "                        bus_stops_gdf = bus_stops_gdf.to_crs(boundary_gdf.crs)\n",
    "                        print(\"Reprojection complete.\")\n",
    "                else:\n",
    "                    print(\"Warning: No valid coordinates found in gstops_df_v1 after cleaning.\")\n",
    "            except Exception as e:\n",
    "                print(f\"*** ERROR converting gstops_df_v1 data: {e} ***\")\n",
    "                bus_stops_gdf = None\n",
    "        else:\n",
    "            print(\"Warning: 'stop_lat' or 'stop_lon' columns not found in gstops_df_v1.\")\n",
    "    else:\n",
    "        print(\"Warning: 'gstops_df_v1' DataFrame not found or is empty. Please load it first.\")\n",
    "\n",
    "\n",
    "    # --- *** PREPARE BUS ROUTES GEODATAFRAME FROM gvariations_df_v1 *** --- \n",
    "    bus_routes_gdf = None # Initialize\n",
    "\n",
    "    if 'gvariations_df_v1' in locals() and isinstance(gvariations_df_v1, pd.DataFrame) and not gvariations_df_v1.empty and \\\n",
    "    'bus_stops_gdf' in locals() and isinstance(bus_stops_gdf, gpd.GeoDataFrame) and not bus_stops_gdf.empty:\n",
    "\n",
    "        print(\"\\nEnriching gvariations_df_v1 with first/last stop Point geometries...\")\n",
    "        \n",
    "        bus_routes_gdf = gvariations_df_v1.copy()\n",
    "        \n",
    "        if 'stop_id' in bus_stops_gdf.columns and 'geometry' in bus_stops_gdf.columns:\n",
    "            \n",
    "            # --- Handle duplicate stop_ids in bus_stops_gdf to get a unique map ---\n",
    "            # A single physical stop_id has one location, regardless of how many route directions use it.\n",
    "            # We keep the first occurrence of each stop_id to get its unique geometry.\n",
    "            bus_stops_gdf_unique_locations = bus_stops_gdf.drop_duplicates(subset=['stop_id'], keep='first')\n",
    "            \n",
    "            # Create the mapping series from this de-duplicated DataFrame\n",
    "            stop_id_to_point_geometry = bus_stops_gdf_unique_locations.set_index('stop_id')['geometry']\n",
    "            \n",
    "        else:\n",
    "            print(\"Error: 'stop_id' or 'geometry' column not found in bus_stops_gdf. Cannot map stop Point geometries.\")\n",
    "            stop_id_to_point_geometry = pd.Series(dtype='object') \n",
    "\n",
    "        # Map first stop Point geometry\n",
    "        bus_routes_gdf['first_stop_point'] = bus_routes_gdf['first_stop_id'].map(stop_id_to_point_geometry)\n",
    "        \n",
    "        # Map last stop Point geometry\n",
    "        bus_routes_gdf['last_stop_point'] = bus_routes_gdf['last_stop_id'].map(stop_id_to_point_geometry)\n",
    "        \n",
    "        num_first_stops_mapped = bus_routes_gdf['first_stop_point'].notna().sum()\n",
    "        num_last_stops_mapped = bus_routes_gdf['last_stop_point'].notna().sum()\n",
    "        \n",
    "        print(f\"Successfully mapped Point geometry for {num_first_stops_mapped} first stops.\")\n",
    "        print(f\"Successfully mapped Point geometry for {num_last_stops_mapped} last stops.\")\n",
    "\n",
    "        # Check if any mappings failed (resulting in NaNs)\n",
    "        if bus_routes_gdf['first_stop_point'].isnull().any() or bus_routes_gdf['last_stop_point'].isnull().any():\n",
    "            print(\"Warning: Some first/last stop points could not be mapped (resulting in NaNs).\")\n",
    "        \n",
    "\n",
    "        print(\"\\n--- bus_routes_gdf (with Point geometries) ---\")\n",
    "        # Display relevant columns to check the mapping\n",
    "        display_cols = ['first_stop_id', 'first_stop_point', 'last_stop_id', 'last_stop_point']\n",
    "        # Add other columns from gvariations_df_v1 if they provide context\n",
    "        if 'route_id' in bus_routes_gdf.columns: display_cols.insert(0, 'route_id')\n",
    "        if 'direction_id' in bus_routes_gdf.columns: display_cols.insert(1, 'direction_id')\n",
    "\n",
    "        print(bus_routes_gdf[display_cols].head())\n",
    "        print(f\"Shape of bus_routes_gdf: {bus_routes_gdf.shape}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nPrerequisite DataFrames ('gvariations_df_v1' or 'bus_stops_gdf') not available or empty. Cannot create bus_routes_gdf.\")\n",
    "\n",
    "\n",
    "      # --- *** CREATE PLACE SUMMARY DATAFRAME *** ---\n",
    "    print(\"\\nCreating DataFrame for Galway Place Names and Coordinates...\")\n",
    "    galway_places_summary_df = None # Initialize\n",
    "    if 'places_poly' in galway_gdfs and not galway_gdfs['places_poly'].empty:\n",
    "        places_data = []\n",
    "        # Check if the 'name' column exists\n",
    "        if 'name' not in galway_gdfs['places_poly'].columns:\n",
    "            print(\"Warning: 'name' column not found in places_poly layer. Cannot extract place names.\")\n",
    "        else:\n",
    "            # Iterate through valid polygons with names\n",
    "            for idx, row in galway_gdfs['places_poly'][galway_gdfs['places_poly']['name'].notna() & galway_gdfs['places_poly'].geometry.is_valid].iterrows():\n",
    "                place_name_val = row['name']; geometry = row.geometry; rep_point = None\n",
    "                # Get representative point (or centroid as fallback)\n",
    "                if hasattr(geometry, 'representative_point'):\n",
    "                    try: rep_point = geometry.representative_point()\n",
    "                    except Exception: rep_point = geometry.centroid # Fallback if representative_point fails\n",
    "                else: rep_point = geometry.centroid # Fallback if method doesn't exist\n",
    "                # Append if point is valid\n",
    "                if rep_point and rep_point.is_valid:\n",
    "                    places_data.append({'place_name': place_name_val,'latitude': rep_point.y,'longitude': rep_point.x})\n",
    "            # Create DataFrame if data was extracted\n",
    "            if places_data:\n",
    "                galway_places_summary_df = pd.DataFrame(places_data)\n",
    "                print(f\"Created DataFrame 'galway_places_summary_df' with {len(galway_places_summary_df)} places.\")\n",
    "                print(galway_places_summary_df.head())\n",
    "            else: print(\"No valid places with names found to create summary DataFrame.\")\n",
    "    else: print(\"Clipped 'places_poly' GeoDataFrame not found or is empty.\")\n",
    "\n",
    "    galway_places_summary_df1 = None # Initialize\n",
    "\n",
    "    if 'galway_places_summary_df' in locals() and isinstance(galway_places_summary_df, pd.DataFrame) and not galway_places_summary_df.empty:\n",
    "        galway_places_summary_df1 = galway_places_summary_df.copy()\n",
    "        if 'place_name' in galway_places_summary_df1.columns:\n",
    "            galway_places_summary_df1 = galway_places_summary_df1.sort_values('place_name').reset_index(drop=True)\n",
    "        else:\n",
    "            print(\"Warning: 'place_name' column not found for sorting. Index will be based on current order.\")\n",
    "\n",
    "        # Create custom indices starting with 'P'\n",
    "        place_indices = [f'P{i+1}' for i in range(len(galway_places_summary_df1))]\n",
    "        galway_places_summary_df1.index = place_indices\n",
    "\n",
    "        print(\"\\nCreated DataFrame 'galway_places_summary_df1' with custom 'P' indices:\")\n",
    "        print(f\"Number of places: {len(galway_places_summary_df1)}\")\n",
    "        print(\"\\nFirst few rows of 'galway_places_summary_df1':\")\n",
    "        print(galway_places_summary_df1.head())\n",
    "    else:\n",
    "        print(\"Cannot create 'galway_places_summary_df1' as 'galway_places_summary_df' is not available or is empty.\")\n",
    "    # --- *** END PLACES SECTION *** ---\n",
    "\n",
    "\n",
    "\n",
    "# --- *** CHECK RAHOON PLACE ID FOR PLOTTING *** ---\n",
    "    rahoon_place_id = None # To store the 'P' index if Rahoon is found\n",
    "    if 'galway_places_summary_df1' in locals() and isinstance(galway_places_summary_df1, pd.DataFrame) and not galway_places_summary_df1.empty:\n",
    "        if 'place_name' in galway_places_summary_df1.columns:\n",
    "            # Search for 'Rahoon' in the 'place_name' column \n",
    "            rahoon_search_results = galway_places_summary_df1[galway_places_summary_df1['place_name'].str.contains('Rahoon', case=False, na=False)]\n",
    "\n",
    "            if not rahoon_search_results.empty:\n",
    "                print(f\"\\n--- Found 'Rahoon' in galway_places_summary_df1 ---\")\n",
    "                rahoon_place_data = rahoon_search_results.iloc[0]\n",
    "                rahoon_place_id = rahoon_place_data.name \n",
    "                print(f\"Place Name: {rahoon_place_data['place_name']}\")\n",
    "                print(f\"Index (ID): {rahoon_place_id}\")\n",
    "                print(f\"Latitude: {rahoon_place_data['latitude']}\")\n",
    "                print(f\"Longitude: {rahoon_place_data['longitude']}\")\n",
    "            else:\n",
    "                print(\"\\nPlace name containing 'Rahoon' not found in galway_places_summary_df1.\")\n",
    "        else:\n",
    "            print(\"\\n'place_name' column not found in galway_places_summary_df1.\")\n",
    "    else:\n",
    "        print(\"\\nDataFrame 'galway_places_summary_df1' not available for searching 'Rahoon'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- *** CREATE BUILDINGS SUMMARY DATAFRAME *** ---\n",
    "    print(\"\\nCreating DataFrame for Galway Buildings with Type and Coordinates...\")\n",
    "    galway_buildings_summary_df = None # Initialize\n",
    "    if 'buildings' in galway_gdfs and not galway_gdfs['buildings'].empty:\n",
    "        buildings_data = []\n",
    "\n",
    "        # Check what columns are available in the buildings layer\n",
    "        print(f\"Available columns in buildings layer: {galway_gdfs['buildings'].columns.tolist()}\")\n",
    "\n",
    "        # Extract building info - name, osm_id, and type (typically in fclass or type column)\n",
    "        for idx, row in galway_gdfs['buildings'][galway_gdfs['buildings'].geometry.is_valid].iterrows():\n",
    "            osm_id = row.get('osm_id', None)\n",
    "            name = row.get('name', None)\n",
    "            building_type = None\n",
    "            for type_col in ['fclass', 'type', 'building']:\n",
    "                if type_col in row and row[type_col] is not None:\n",
    "                    building_type = row[type_col]; break\n",
    "            try:\n",
    "                centroid = row.geometry.centroid\n",
    "                if centroid and centroid.is_valid:\n",
    "                    buildings_data.append({\n",
    "                        'building_name': name, 'osm_id': osm_id, 'building_type': building_type,\n",
    "                        'latitude': centroid.y, 'longitude': centroid.x\n",
    "                    })\n",
    "            except Exception as e: print(f\"Error calculating centroid for building {osm_id}: {e}\")\n",
    "\n",
    "        if buildings_data:\n",
    "            galway_buildings_summary_df = pd.DataFrame(buildings_data)\n",
    "            print(f\"Created DataFrame 'galway_buildings_summary_df' with {len(galway_buildings_summary_df)} buildings.\")\n",
    "            print(galway_buildings_summary_df.head())\n",
    "        else: print(\"No valid building data found to create summary DataFrame.\")\n",
    "    else: print(\"Clipped 'buildings' GeoDataFrame not found or is empty.\")\n",
    "\n",
    "    # --- *** REFINE BUILDING SUMMARY DATAFRAME *** ---\n",
    "    galway_buildings_summary_df1 = None # Initialize\n",
    "    if galway_buildings_summary_df is not None:\n",
    "        galway_buildings_summary_df1 = galway_buildings_summary_df[galway_buildings_summary_df['building_name'].notnull()].copy()\n",
    "        galway_buildings_summary_df1 = galway_buildings_summary_df1.sort_values('building_name')\n",
    "        building_indices = [f'B{i+1}' for i in range(len(galway_buildings_summary_df1))]\n",
    "        galway_buildings_summary_df1.index = building_indices\n",
    "        print(\"\\nCreated filtered DataFrame 'galway_buildings_summary_df1' with named buildings:\")\n",
    "        print(f\"Number of named buildings: {len(galway_buildings_summary_df1)}\")\n",
    "        print(\"\\nFirst few rows of filtered DataFrame:\")\n",
    "        print(galway_buildings_summary_df1.head())\n",
    "    else: print(\"Cannot create filtered DataFrame as galway_buildings_summary_df is None\")\n",
    "    # --- *** END BUILDINGS SECTION *** ---\n",
    "\n",
    "\n",
    "    # --- *** PLOTTING CLIPPED GALWAY DATA *** ---\n",
    "    print(\"\\nPlotting clipped Galway map layers...\")\n",
    "    fig, ax = plt.subplots(figsize=(18, 18), facecolor='white', dpi=250)\n",
    "\n",
    "    # Define base colors\n",
    "    color_water = '#a8dff5'; color_land = '#f2f4f6'; color_parks = '#cceac4'\n",
    "    color_buildings_osm = '#d8cabc' \n",
    "    color_roads = '#aaaaaa'; color_rail = '#a0a0a0';color_place_text = '#36454F'  \n",
    "    \n",
    "    # Define bus stop color\n",
    "    color_bus_stops_blue = '#1E90FF' \n",
    "\n",
    "    # Set background\n",
    "    ax.set_facecolor(color_land)\n",
    "\n",
    "    # Define approximate z-orders\n",
    "    zorder_landuse=1; zorder_water_poly=2; zorder_parks=3; zorder_buildings_layer=4 # General buildings layer\n",
    "    zorder_waterways=5; zorder_railways=6; zorder_roads=7;\n",
    "    zorder_bus_stops_plot = 8    # Z-order for general bus stops\n",
    "    zorder_place_text = 9        # Z-order for general place name labels\n",
    "\n",
    "    # Z-orders for the specific B422 building highlight - Portershed\n",
    "    zorder_building_b422_point = 10  \n",
    "    zorder_building_b422_text = 11  \n",
    "\n",
    "    # Z-orders for the specific 'Rahoon' place highlight\n",
    "    zorder_rahoon_place_point = 10 \n",
    "    zorder_rahoon_place_text = 11  \n",
    "\n",
    "\n",
    "    zorder_boundary = 12   # Boundary should be having highest zorder to frame everything\n",
    "    \n",
    "\n",
    "    # Plot base layers\n",
    "    if 'landuse' in galway_gdfs: galway_gdfs['landuse'].plot(ax=ax, column='fclass', categorical=True, cmap='Pastel2', alpha=0.4, zorder=zorder_landuse)\n",
    "    if 'water_poly' in galway_gdfs: galway_gdfs['water_poly'].plot(ax=ax, color=color_water, edgecolor='none', zorder=zorder_water_poly)\n",
    "    if 'landuse' in galway_gdfs and 'fclass' in galway_gdfs['landuse'].columns:\n",
    "        parks_gdf = galway_gdfs['landuse'][galway_gdfs['landuse']['fclass'] == 'park']\n",
    "        if not parks_gdf.empty: parks_gdf.plot(ax=ax, color=color_parks, edgecolor='none', zorder=zorder_parks)\n",
    "    if 'buildings' in galway_gdfs: galway_gdfs['buildings'].plot(ax=ax, facecolor=color_buildings_osm, alpha=0.7, lw=0.5, edgecolor=color_buildings_osm, zorder=zorder_buildings_layer)\n",
    "    if 'waterways' in galway_gdfs: galway_gdfs['waterways'].plot(ax=ax, color=color_water, linewidth=1.0, zorder=zorder_waterways)\n",
    "    if 'railways' in galway_gdfs:\n",
    "        galway_gdfs['railways'].plot(ax=ax, color='#ffffff', linewidth=2.0, linestyle='-', zorder=zorder_railways)\n",
    "        galway_gdfs['railways'].plot(ax=ax, color=color_rail, linewidth=1.0, linestyle='-', zorder=zorder_railways + 0.1)\n",
    "    if 'roads' in galway_gdfs: galway_gdfs['roads'].plot(ax=ax, color=color_roads, linewidth=0.8, zorder=zorder_roads)\n",
    "\n",
    "    # --- Plot ALL Bus Stops from gstops_df_v1 ---\n",
    "    if bus_stops_gdf is not None and not bus_stops_gdf.empty:\n",
    "        bus_stops_gdf.plot(\n",
    "            ax=ax,\n",
    "            color=color_bus_stops_blue, \n",
    "            marker='o',\n",
    "            markersize=15,             \n",
    "            edgecolor='black',        \n",
    "            linewidth=0.5,\n",
    "            alpha=0.9,\n",
    "            zorder=zorder_bus_stops_plot, \n",
    "            label='Bus Stops (All)'\n",
    "        )\n",
    "        print(f\"Plotted {len(bus_stops_gdf)} bus stops from gstops_df_v1 as blue dots.\")\n",
    "    else:\n",
    "        print(\"No bus stops from gstops_df_v1 to plot.\")\n",
    "\n",
    "\n",
    "    # --- Plot Place Names (No Circles) ---\n",
    "    if galway_places_summary_df is not None and not galway_places_summary_df.empty:\n",
    "        print(f\"Plotting {len(galway_places_summary_df)} place names...\")\n",
    "        plotted_place_names_map = set()\n",
    "        for idx, row in galway_places_summary_df.iterrows():\n",
    "            label = row['place_name']; point_x = row['longitude']; point_y = row['latitude']\n",
    "            if label not in plotted_place_names_map:\n",
    "                ax.text(point_x, point_y + 0.0002, label, fontsize=8, color=color_place_text,\n",
    "                        ha='center', va='bottom', zorder=zorder_place_text, fontweight='normal',\n",
    "                        path_effects=[matplotlib.patheffects.withStroke(linewidth=1, foreground='w')])\n",
    "                plotted_place_names_map.add(label)\n",
    "        print(\"Place names plotted.\")\n",
    "\n",
    "    # --- *** PLOT B422 BUILDING - PORTERSHED *** ---\n",
    "    if 'galway_buildings_summary_df1' in locals() and galway_buildings_summary_df1 is not None and not galway_buildings_summary_df1.empty:\n",
    "        building_point_color = '#FF5733' # Orange\n",
    "        building_text_color = '#000000'  # Black\n",
    "        plotted_b422 = False\n",
    "        # Ensure B422 exists in your dataframe's index\n",
    "        if 'B422' in galway_buildings_summary_df1.index:\n",
    "            row = galway_buildings_summary_df1.loc['B422']\n",
    "            point_x = row['longitude']\n",
    "            point_y = row['latitude']\n",
    "            building_name = row['building_name']\n",
    "            \n",
    "            # Plot orange circle for B422\n",
    "            plt.scatter(point_x, point_y, s=60, color=building_point_color, edgecolor='black', # Increased size (s=60)\n",
    "                        linewidth=1, alpha=0.9, zorder=zorder_building_b422_point, label=f'Building: {building_name}')\n",
    "            \n",
    "            # Plot name label for B422\n",
    "            ax.text(point_x, point_y + 0.0003, building_name, fontsize=7, color=building_text_color, \n",
    "                    ha='center', va='bottom', zorder=zorder_building_b422_text, fontweight='bold',\n",
    "                    path_effects=[matplotlib.patheffects.withStroke(linewidth=1, foreground='white')])\n",
    "            plotted_b422 = True\n",
    "            print(f\"Plotted orange circle and name label for building B422 ('{building_name}').\")\n",
    "        else:\n",
    "            print(\"Building B422 not found in the DataFrame 'galway_buildings_summary_df1'.\")\n",
    "    else:\n",
    "        print(\"DataFrame 'galway_buildings_summary_df1' not available for plotting B422.\")\n",
    "    # --- *** END OF B422 PLOTTING CODE *** ---   \n",
    "\n",
    "\n",
    "\n",
    "    # --- *** PLOT SPECIFIC PLACE 'RAHOON' *** ---\n",
    "    if 'rahoon_place_id' in locals() and rahoon_place_id is not None and \\\n",
    "       'galway_places_summary_df1' in locals() and galway_places_summary_df1 is not None and \\\n",
    "       not galway_places_summary_df1.empty:\n",
    "\n",
    "        if rahoon_place_id in galway_places_summary_df1.index:\n",
    "            place_row = galway_places_summary_df1.loc[rahoon_place_id]\n",
    "            point_x = place_row['longitude']\n",
    "            point_y = place_row['latitude']\n",
    "            place_name_label = place_row['place_name'] \n",
    "\n",
    "            place_point_color = '#9400D3' # Dark Violet \n",
    "            place_text_color = '#000000'   # Black\n",
    "\n",
    "            # Plot distinct circle for 'Rahoon'\n",
    "            plt.scatter(point_x, point_y, s=70, color=place_point_color, edgecolor='black', \n",
    "                        linewidth=1, alpha=0.9, zorder=zorder_rahoon_place_point, label=f'Place: {place_name_label}')\n",
    "\n",
    "            # Plot name label for 'Rahoon'\n",
    "            ax.text(point_x, point_y + 0.00035, place_name_label, fontsize=7.5, color=place_text_color,\n",
    "                    ha='center', va='bottom', zorder=zorder_rahoon_place_text, fontweight='bold',\n",
    "                    path_effects=[matplotlib.patheffects.withStroke(linewidth=1, foreground='white')])\n",
    "            print(f\"Plotted distinct circle and name label for place: '{place_name_label}' (ID: {rahoon_place_id}).\")\n",
    "        else:\n",
    "            print(f\"Place with ID '{rahoon_place_id}' (expected to be Rahoon) not found in galway_places_summary_df1.index for plotting.\")\n",
    "    else:\n",
    "        print(\"Rahoon was not identified or 'galway_places_summary_df1' is not available for plotting specific place.\")\n",
    "    # --- *** END OF 'RAHOON' PLOTTING CODE *** ---\n",
    "\n",
    "\n",
    "    # Plot boundary outline for context last\n",
    "    boundary_gdf.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.5, linestyle='--', zorder=zorder_boundary)\n",
    "\n",
    "    # --- Set Map Bounds ---\n",
    "    if 'roads' in galway_gdfs and not galway_gdfs['roads'].empty:\n",
    "        minx, miny, maxx, maxy = galway_gdfs['roads'].total_bounds\n",
    "    else:\n",
    "        minx, miny, maxx, maxy = boundary_gdf.total_bounds\n",
    "    margin_factor = 0.02\n",
    "    margin_x = (maxx - minx) * margin_factor\n",
    "    margin_y = (maxy - miny) * margin_factor\n",
    "    ax.set_xlim(minx - margin_x, maxx + margin_x)\n",
    "    ax.set_ylim(miny - margin_y, maxy + margin_y)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    # Final plot adjustments\n",
    "    ax.set_title(f\"Galway Map with Bus Stops (from gstops_df_v1)\", color='black', fontsize=16)\n",
    "    plt.legend(loc='upper right') # add a legend\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    " \n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n--- File Error ---\\n{e}\\nPlease ensure file paths are correct.\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n--- Import Error Occurred ---\\nError: {e}\\nPlease ensure required libraries are installed.\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n--- Value Error ---\\n{e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An Unexpected Error Occurred ---\\nError: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Stop and Route Name Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Abbreviation Expansion and Parentheses Removal Function ---\n",
    "def expand_abbr_and_remove_paren(text_to_process):\n",
    "    if pd.isna(text_to_process):\n",
    "        return text_to_process\n",
    "\n",
    "    current_text = str(text_to_process) \n",
    "    \n",
    "    abbreviations = {\n",
    "        r'\\brd\\b': 'road',\n",
    "        r'\\bst\\b': 'street',\n",
    "        r'\\blwr\\b': 'lower',\n",
    "        r'\\bav\\b': 'avenue',\n",
    "        r'\\bave\\b': 'avenue',\n",
    "        r'\\bopp\\b': 'opposite',\n",
    "        r'\\bind est\\b': 'industrial estate',\n",
    "    }\n",
    "    \n",
    "    expanded_text = current_text\n",
    "    for abbr, expansion in abbreviations.items():\n",
    "        expanded_text = re.sub(abbr, expansion, expanded_text)\n",
    "    \n",
    "    text_after_paren_open_removed = re.sub(r'\\s*\\(\\s*', ' ', expanded_text) \n",
    "    text_after_paren_close_removed = re.sub(r'\\s*\\)\\s*', ' ', text_after_paren_open_removed)\n",
    "    \n",
    "    # Consolidate multiple spaces into one\n",
    "    final_text = re.sub(r'\\s+', ' ', text_after_paren_close_removed).strip()\n",
    "        \n",
    "    return final_text\n",
    "\n",
    "# --- Prepare bus_stops_gdf ---\n",
    "if 'bus_stops_gdf' in locals() and isinstance(bus_stops_gdf, pd.DataFrame) and not bus_stops_gdf.empty:\n",
    "    if 'stop_name' in bus_stops_gdf.columns:    \n",
    "        bus_stops_gdf['stop_name_norm_explicit'] = bus_stops_gdf['stop_name'].astype(str).str.lower().str.strip()\n",
    "        bus_stops_gdf['stop_name_norm_expanded'] = bus_stops_gdf['stop_name_norm_explicit'].apply(expand_abbr_and_remove_paren)\n",
    "        print(\"bus_stops_gdf prepared.\")\n",
    "    else:\n",
    "        print(\"Error: 'stop_name' column not found in bus_stops_gdf.\")\n",
    "else:\n",
    "    print(\"Error: bus_stops_gdf is not defined or is empty.\")\n",
    "\n",
    "# --- Prepare bus_timetables ---\n",
    "if 'bus_timetables' in locals() and isinstance(bus_timetables, pd.DataFrame) and not bus_timetables.empty:\n",
    "    if 'ROUTE' in bus_timetables.columns: \n",
    "        bus_timetables['ROUTE_norm_explicit'] = bus_timetables['ROUTE'].astype(str).str.lower().str.strip()\n",
    "        bus_timetables['ROUTE_norm_expanded'] = bus_timetables['ROUTE_norm_explicit'].apply(expand_abbr_and_remove_paren)\n",
    "        \n",
    "        print(\"bus_timetables prepared.\")\n",
    "    else:\n",
    "        print(\"Error: 'ROUTE' column not found in bus_timetables.\")\n",
    "else:\n",
    "    print(\"\\nError: bus_timetables DataFrame is not defined or is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Bus Stop Matching and Route Mapping  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MATCHING FUNCTION (Exact -> Fuzzy -> Token Overlap) ---\n",
    "def find_stop_id_expanded_match(route_norm_expanded_to_match, stops_df):\n",
    "    if pd.isna(route_norm_expanded_to_match):\n",
    "        return None, \"Input ROUTE_norm_expanded is NaN\", None, np.nan, np.nan\n",
    "\n",
    "    if not isinstance(stops_df, pd.DataFrame) or stops_df.empty:\n",
    "        return None, \"stops_df (bus_stops_gdf) is invalid or empty\", None, np.nan, np.nan\n",
    "\n",
    "    required_cols = ['stop_name_norm_expanded', 'stop_id', 'stop_name']\n",
    "    if not all(col in stops_df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in stops_df.columns]\n",
    "        return None, f\"Missing required columns in stops_df: {missing}\", None, np.nan, np.nan\n",
    "\n",
    "    matched_stop_id = None\n",
    "    match_method = \"No Initial Match\" # Default if no stages match\n",
    "    matched_original_stop_name_in_gdf = None\n",
    "    fuzz_ratio_score = np.nan\n",
    "    fuzz_wratio_score = np.nan\n",
    "\n",
    "    # --- Stage 1: Exact match ---\n",
    "    exact_match_gdf = stops_df[stops_df['stop_name_norm_expanded'] == route_norm_expanded_to_match]\n",
    "    if not exact_match_gdf.empty:\n",
    "        matched_stop_id = exact_match_gdf.iloc[0]['stop_id']\n",
    "        matched_original_stop_name_in_gdf = exact_match_gdf.iloc[0]['stop_name']\n",
    "        match_method = \"Exact Expanded Name\" + (\" (Multiple GDF matches, took first)\" if len(exact_match_gdf) > 1 else \"\")\n",
    "        fuzz_ratio_score, fuzz_wratio_score = 100, 100\n",
    "        return matched_stop_id, match_method, matched_original_stop_name_in_gdf, fuzz_ratio_score, fuzz_wratio_score\n",
    "\n",
    "    # --- Stage 2: Fuzzy match ---\n",
    "    choices = stops_df['stop_name_norm_expanded'].dropna().tolist()\n",
    "    if choices:\n",
    "        best_match_wratio_tuple = process.extractOne(route_norm_expanded_to_match, choices, scorer=fuzz.WRatio, score_cutoff=88)\n",
    "        if best_match_wratio_tuple:\n",
    "            best_matched_expanded_name_in_gdf = best_match_wratio_tuple[0]\n",
    "            fuzz_wratio_score = best_match_wratio_tuple[1]\n",
    "            fuzz_ratio_score = fuzz.ratio(route_norm_expanded_to_match, best_matched_expanded_name_in_gdf)\n",
    "            gdf_row_for_best_match = stops_df[stops_df['stop_name_norm_expanded'] == best_matched_expanded_name_in_gdf]\n",
    "            if not gdf_row_for_best_match.empty:\n",
    "                matched_stop_id = gdf_row_for_best_match.iloc[0]['stop_id']\n",
    "                matched_original_stop_name_in_gdf = gdf_row_for_best_match.iloc[0]['stop_name']\n",
    "                match_method = f\"Fuzzy Expanded Name (WRatio: {fuzz_wratio_score:.0f})\"\n",
    "                return matched_stop_id, match_method, matched_original_stop_name_in_gdf, fuzz_ratio_score, fuzz_wratio_score\n",
    "            else: # Should be rare\n",
    "                match_method = \"Fuzzy Match Found but GDF Row Missing\"\n",
    "                return None, match_method, None, fuzz_ratio_score, fuzz_wratio_score\n",
    "    \n",
    "    # --- Stage 3: Token-based Overlap Match (if no exact or fuzzy match) ---\n",
    "    if matched_stop_id is None and isinstance(route_norm_expanded_to_match, str) and len(route_norm_expanded_to_match.strip()) > 0:\n",
    "        route_tokens = set(route_norm_expanded_to_match.split())\n",
    "        if not route_tokens:\n",
    "             return None, \"No Final Match (Empty Route Tokens)\", None, np.nan, np.nan\n",
    "\n",
    "        best_overlap_score = 0\n",
    "        candidate_stop_id = None\n",
    "        candidate_original_name = None\n",
    "        best_gdf_name_for_token_match = None # For tie-breaking\n",
    "\n",
    "        # Minimum common tokens required for this type of match\n",
    "        min_required_common_tokens = 1 \n",
    "\n",
    "        for index, row in stops_df.iterrows():\n",
    "            gdf_stop_name_expanded = row['stop_name_norm_expanded']\n",
    "            if pd.isna(gdf_stop_name_expanded):\n",
    "                continue\n",
    "            \n",
    "            gdf_tokens = set(gdf_stop_name_expanded.split())\n",
    "            if not gdf_tokens:\n",
    "                continue\n",
    "                \n",
    "            common_tokens = route_tokens.intersection(gdf_tokens)\n",
    "            current_overlap_score = len(common_tokens)\n",
    "            \n",
    "            if current_overlap_score >= min_required_common_tokens:\n",
    "                if current_overlap_score > best_overlap_score:\n",
    "                    best_overlap_score = current_overlap_score\n",
    "                    candidate_stop_id = row['stop_id']\n",
    "                    candidate_original_name = row['stop_name']\n",
    "                    best_gdf_name_for_token_match = gdf_stop_name_expanded\n",
    "                elif current_overlap_score == best_overlap_score:\n",
    "                    # Tie-breaking: prefer shorter GDF name if overlap score is the same\n",
    "                    if candidate_original_name is None or (best_gdf_name_for_token_match and len(gdf_stop_name_expanded) < len(best_gdf_name_for_token_match)):\n",
    "                        candidate_stop_id = row['stop_id']\n",
    "                        candidate_original_name = row['stop_name']\n",
    "                        best_gdf_name_for_token_match = gdf_stop_name_expanded\n",
    "                            \n",
    "        if candidate_stop_id is not None:\n",
    "            matched_stop_id = candidate_stop_id\n",
    "            matched_original_stop_name_in_gdf = candidate_original_name\n",
    "            match_method = f\"Token Overlap Match (Score: {best_overlap_score})\"\n",
    "            # Fuzzy scores are not applicable here, remain np.nan\n",
    "            return matched_stop_id, match_method, matched_original_stop_name_in_gdf, np.nan, np.nan\n",
    "\n",
    "    # If no match from any stage\n",
    "    final_match_method = \"No Final Match\" if match_method == \"No Initial Match\" else match_method\n",
    "    return None, final_match_method, None, np.nan, np.nan\n",
    "\n",
    "\n",
    "# --- APPLYING THE MATCHING TO bus_timetables ---\n",
    "if ('bus_timetables' in locals() and isinstance(bus_timetables, pd.DataFrame) and not bus_timetables.empty and\n",
    "    'ROUTE_norm_expanded' in bus_timetables.columns and\n",
    "    'bus_stops_gdf' in locals() and isinstance(bus_stops_gdf, pd.DataFrame) and not bus_stops_gdf.empty and\n",
    "    'stop_name_norm_expanded' in bus_stops_gdf.columns and 'stop_name' in bus_stops_gdf.columns and 'stop_id' in bus_stops_gdf.columns):\n",
    "\n",
    "    print(\"\\nMapping ROUTE_norm_expanded to stop_id (Exact -> Fuzzy -> Token Overlap)...\")\n",
    "    \n",
    "    match_results_tuples = bus_timetables['ROUTE_norm_expanded'].apply(\n",
    "        lambda x: find_stop_id_expanded_match(x, bus_stops_gdf.copy()) # Pass a copy of bus_stops_gdf\n",
    "    )\n",
    "    \n",
    "    bus_timetables['stop_id_mapped'] = [res[0] for res in match_results_tuples]\n",
    "    bus_timetables['match_method'] = [res[1] for res in match_results_tuples]\n",
    "    bus_timetables['matched_stop_name_in_gdf'] = [res[2] for res in match_results_tuples]\n",
    "    bus_timetables['fuzz_ratio_score'] = [res[3] for res in match_results_tuples] \n",
    "    bus_timetables['fuzz_wratio_score'] = [res[4] for res in match_results_tuples]\n",
    "\n",
    "    print(\"Mapping complete.\")\n",
    "    print(\"\\n--- bus_timetables with mapped stop_id (Exact -> Fuzzy -> Token Overlap) ---\")\n",
    "    display_cols = ['ROUTE', 'ROUTE_norm_expanded', 'stop_id_mapped', 'match_method', 'matched_stop_name_in_gdf', 'fuzz_ratio_score', 'fuzz_wratio_score']\n",
    "    if 'route_id' in bus_timetables.columns: # Add route_id if it exists\n",
    "        if 'ROUTE_norm_expanded' in display_cols and 'route_id' not in display_cols:\n",
    "             display_cols.insert(display_cols.index('ROUTE_norm_expanded') + 1, 'route_id')\n",
    "        elif 'route_id' not in display_cols:\n",
    "            display_cols.append('route_id')\n",
    "            \n",
    "    final_display_cols = [col for col in display_cols if col in bus_timetables.columns]\n",
    "    \n",
    "    print(\"\\nMatch Method Distribution:\")\n",
    "    \n",
    "    unmapped_count = bus_timetables['stop_id_mapped'].isna().sum()\n",
    "    print(f\"\\nNumber of ROUTEs not mapped to a stop_id: {unmapped_count} out of {len(bus_timetables)}\")\n",
    "    \n",
    "    if unmapped_count > 0:\n",
    "        print(\"Sample of unmapped ROUTEs:\")\n",
    "        unmapped_sample_cols = ['ROUTE', 'ROUTE_norm_expanded', 'match_method']\n",
    "        if 'route_id' in bus_timetables.columns: unmapped_sample_cols.append('route_id')\n",
    "        unmapped_sample_cols = [col for col in unmapped_sample_cols if col in bus_timetables.columns]\n",
    "        if unmapped_sample_cols:\n",
    "            display(bus_timetables[bus_timetables['stop_id_mapped'].isna()][unmapped_sample_cols].head(10))\n",
    "            \n",
    "    print(\"\\nReview Token Overlap Matches:\")\n",
    "    if 'match_method' in bus_timetables.columns:\n",
    "        token_overlap_review = bus_timetables[bus_timetables['match_method'].str.contains(\"Token Overlap\", na=False)][final_display_cols]\n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot perform mapping. Prerequisites not met (check DataFrames and required columns).\")\n",
    "\n",
    "\n",
    "# Replace empty strings in 'stop_id_mapped' with pd.NA (or np.nan)\n",
    "bus_timetables['stop_id_mapped'] = bus_timetables['stop_id_mapped'].replace('', pd.NA)\n",
    "\n",
    "# Drop rows where 'stop_id_mapped' is NA\n",
    "bus_timetables = bus_timetables.dropna(subset=['stop_id_mapped'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('galway_places_summary_df1')\n",
    "display(galway_places_summary_df1.head(2))\n",
    "print('galway_buildings_summary_df1')\n",
    "display(galway_buildings_summary_df1.head(2))\n",
    "print('bus_stops_gdf')\n",
    "display(bus_stops_gdf.head(2))\n",
    "print('bus_timetables')\n",
    "display(bus_timetables.head(2))\n",
    "print('bus_routes_gdf')\n",
    "display(bus_routes_gdf.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Network Graph Structure : multi-modal public transportation graph for Galway\n",
    "\n",
    "Network structure with nodes (places, buildings, bus stops) and edges (walking connections, bus routes)\n",
    "\n",
    "add places, buildings and bus stops Nodes \n",
    "\n",
    "add Access/Egress Edges - walking connections i.e., place-TO-nearby-bus-stop / nearby-bus-stop-TO-building\n",
    "\n",
    "add Directed Transit Edges - bus route between consecutive stops\n",
    "\n",
    "**User Input:** MAX_ACCESS_DISTANCE_METERS = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Haversine Distance ---\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    try:\n",
    "        lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(radians, [float(lat1), float(lon1), float(lat2), float(lon2)])\n",
    "    except (ValueError, TypeError):\n",
    "        return float('inf')\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c * 1000\n",
    "\n",
    "\n",
    "# --- 1. Initialize Graph ---\n",
    "G = nx.DiGraph()\n",
    "print(\"Graph initialized.\")\n",
    "\n",
    "# --- Step 1a: Add Place Nodes ---\n",
    "print(\"\\nAdding place nodes (general POIs)...\")\n",
    "for index, row in galway_places_summary_df1.iterrows():\n",
    "    place_node_id = row['place_name']\n",
    "    G.add_node(place_node_id, type='place', name=row['place_name'], latitude=row['latitude'], longitude=row['longitude'])\n",
    "print(f\"Nodes after general places: {G.number_of_nodes()}\")\n",
    "\n",
    "# --- Step 1b: Add Building Nodes---\n",
    "print(\"\\nAdding building nodes...\")\n",
    "for index, row in galway_buildings_summary_df1.iterrows():\n",
    "    building_node_id = row['building_name']\n",
    "    G.add_node(building_node_id, type='building', name=row['building_name'], osm_id=row.get('osm_id'),\n",
    "               building_type=row.get('building_type'), latitude=row['latitude'], longitude=row['longitude'])\n",
    "print(f\"Nodes after adding buildings: {G.number_of_nodes()}\")\n",
    "\n",
    "# --- Step 1c: Add Unique Bus Stop Nodes ---\n",
    "print(\"\\nAdding unique bus stop nodes...\")\n",
    "added_stop_ids = set()\n",
    "for index, row in bus_stops_gdf.iterrows():\n",
    "    stop_id = row['stop_id']\n",
    "    if stop_id not in added_stop_ids:\n",
    "        G.add_node(stop_id, type='bus_stop', name=row['stop_name'], latitude=row['stop_lat'], longitude=row['stop_lon'],\n",
    "                   direction=row.get('direction'), original_route_id=row.get('route_id'), geometry=row.get('geometry'),\n",
    "                   norm_explicit=row.get('stop_name_norm_explicit'), norm_expanded=row.get('stop_name_norm_expanded'))\n",
    "        added_stop_ids.add(stop_id)\n",
    "print(f\"Total nodes after bus stops: {G.number_of_nodes()}\")\n",
    "\n",
    "# --- Step 2: Add Access/Egress Edges\n",
    "print(\"\\nAdding access/egress edges...\")\n",
    "MAX_ACCESS_DISTANCE_METERS = 800\n",
    "\n",
    "access_edge_count = 0\n",
    "place_nodes_data = {node_id: data for node_id, data in G.nodes(data=True) if data.get('type') == 'place'}\n",
    "building_nodes_data = {node_id: data for node_id, data in G.nodes(data=True) if data.get('type') == 'building'}\n",
    "bus_stop_nodes_data = {node_id: data for node_id, data in G.nodes(data=True) if data.get('type') == 'bus_stop'}\n",
    "for place_node_id, place_data in place_nodes_data.items():\n",
    "    place_lat = place_data.get('latitude'); place_lon = place_data.get('longitude')\n",
    "    if place_lat is None or place_lon is None: continue\n",
    "    for stop_node_id, stop_data in bus_stop_nodes_data.items():\n",
    "        stop_lat = stop_data.get('latitude'); stop_lon = stop_data.get('longitude')\n",
    "        if stop_lat is None or stop_lon is None: continue\n",
    "        walking_distance_m = haversine(place_lat, place_lon, stop_lat, stop_lon)\n",
    "        if walking_distance_m <= MAX_ACCESS_DISTANCE_METERS:\n",
    "            edge_attrs = {'type':'access_egress', 'mode':'walk', 'distance_m': walking_distance_m}\n",
    "            G.add_edge(place_node_id, stop_node_id, **edge_attrs); G.add_edge(stop_node_id, place_node_id, **edge_attrs)\n",
    "            access_edge_count += 2;\n",
    "\n",
    "for building_node_id, building_data in building_nodes_data.items():\n",
    "    building_lat = building_data.get('latitude'); building_lon = building_data.get('longitude')\n",
    "    if building_lat is None or building_lon is None: continue\n",
    "    for stop_node_id, stop_data in bus_stop_nodes_data.items():\n",
    "        stop_lat = stop_data.get('latitude'); stop_lon = stop_data.get('longitude')\n",
    "        if stop_lat is None or stop_lon is None: continue\n",
    "        walking_distance_m = haversine(building_lat, building_lon, stop_lat, stop_lon)\n",
    "        if walking_distance_m <= MAX_ACCESS_DISTANCE_METERS:\n",
    "            edge_attrs = {'type':'access_egress', 'mode':'walk', 'distance_m': walking_distance_m}\n",
    "            G.add_edge(building_node_id, stop_node_id, **edge_attrs); G.add_edge(stop_node_id, building_node_id, **edge_attrs)\n",
    "            access_edge_count += 2;\n",
    "print(f\"Added {access_edge_count} access/egress edges in total.\")\n",
    "\n",
    "# --- Step 3: Add Directed Transit Edges ---\n",
    "print(\"\\nAdding directed transit edges...\")\n",
    "transit_edge_count = 0\n",
    "valid_graph_stop_node_ids = {node_id for node_id, data in G.nodes(data=True) if data.get('type') == 'bus_stop'}\n",
    "print(f\"Debug: Found {len(valid_graph_stop_node_ids)} valid bus_stop nodes in the graph for transit edges.\")\n",
    "#\n",
    "print(f\"Debug: bus_timetables has {len(bus_timetables)} rows for transit edge creation.\")\n",
    "\n",
    "for route_id_timetable, group in bus_timetables.groupby('route_id'): # Using bus_timetables\n",
    "    route_stops = group.sort_values(by='stop_order_on_route')\n",
    "    for i in range(len(route_stops) - 1):\n",
    "        from_stop_id_mapped = route_stops.iloc[i]['stop_id_mapped']\n",
    "        to_stop_id_mapped = route_stops.iloc[i+1]['stop_id_mapped']\n",
    "        from_node_exists = from_stop_id_mapped in valid_graph_stop_node_ids\n",
    "        to_node_exists = to_stop_id_mapped in valid_graph_stop_node_ids\n",
    "\n",
    "        if from_node_exists and to_node_exists:\n",
    "            from_stop_node_data = G.nodes[from_stop_id_mapped]\n",
    "            to_stop_node_data = G.nodes[to_stop_id_mapped]\n",
    "            from_lat, from_lon = from_stop_node_data.get('latitude'), from_stop_node_data.get('longitude')\n",
    "            to_lat, to_lon = to_stop_node_data.get('latitude'), to_stop_node_data.get('longitude')\n",
    "\n",
    "            if None not in [from_lat, from_lon, to_lat, to_lon]:\n",
    "                segment_distance_m = haversine(from_lat, from_lon, to_lat, to_lon)\n",
    "                edge_attrs = {'type':'transit', 'route_id':route_id_timetable, 'hop_count':1, 'distance_m':segment_distance_m}\n",
    "                G.add_edge(from_stop_id_mapped, to_stop_id_mapped, **edge_attrs)\n",
    "                transit_edge_count += 1\n",
    "print(f\"Added {transit_edge_count} directed transit edges.\")\n",
    "\n",
    "# --- Final Graph Summary ---\n",
    "print(\"\\n--- Graph Construction Complete ---\")\n",
    "print(f\"Total nodes in graph: {G.number_of_nodes()}\")\n",
    "print(f\"Total edges in graph: {G.number_of_edges()}\")\n",
    "node_types_list = [data.get('type', 'Unknown') for node, data in G.nodes(data=True)]\n",
    "print(\"\\nNode type counts:\\n\", pd.Series(node_types_list).value_counts())\n",
    "edge_summary = []\n",
    "for u, v, data in G.edges(data=True):\n",
    "    edge_type = data.get('type', 'Unknown')\n",
    "    edge_summary.append(f\"{edge_type}_{data.get('mode', '')}\" if edge_type == 'access_egress' else edge_type)\n",
    "print(\"\\nEdge type counts:\\n\", pd.Series(edge_summary).value_counts())\n",
    "\n",
    "# --- check edges with data ---\n",
    "print(\"\\n--- Explicit check of G.edges(data=True) ---\")\n",
    "all_edges_with_data = list(G.edges(data=True))\n",
    "if not all_edges_with_data: print(\"G.edges(data=True) is EMPTY.\")\n",
    "else:\n",
    "    print(f\"Found {len(all_edges_with_data)} edges with data. Sample:\")\n",
    "    for i, edge_tuple in enumerate(all_edges_with_data): \n",
    "        print(edge_tuple)\n",
    "        if i >= 2: \n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Define Origin (\"Place\"), Destination (\"Building\") location and get their nearby bus stops\n",
    "\n",
    "**User input:**\n",
    "place_of_interest_rahoon = \"Rahoon\"; \n",
    "place_of_interest_portershed = \"Portershed a Dó\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking for exact node ID: 'Rahoon' ---\n",
      "Node 'Rahoon' FOUND.\n",
      "Attributes: {'type': 'place', 'name': 'Rahoon', 'latitude': 53.27716555, 'longitude': -9.089465645095348}\n",
      "\n",
      "--- Checking for exact node ID: 'Portershed a Dó' ---\n",
      "Node 'Portershed a Dó' FOUND.\n",
      "Attributes: {'type': 'building', 'name': 'Portershed a Dó', 'osm_id': '175583929', 'building_type': 'building', 'latitude': 53.27365480436892, 'longitude': -9.053651919263224}\n",
      "Using proximity threshold: 800 meters.\n",
      "Proximity Threshold: 800 meters.\n",
      "place_nearby_stops : [{'stop_id': '84605257301', 'distance_m': 729.9969032122906}, {'stop_id': '8460B5255001', 'distance_m': 601.2206968680148}, {'stop_id': '8460B5255201', 'distance_m': 537.305770665661}, {'stop_id': '8460B5255401', 'distance_m': 796.4924585739333}, {'stop_id': '8460B5256901', 'distance_m': 726.4626934218753}, {'stop_id': '8460B5257001', 'distance_m': 767.3727018826424}, {'stop_id': '8460B5257101', 'distance_m': 691.3026353985596}, {'stop_id': '8460B5257201', 'distance_m': 545.8683636637078}, {'stop_id': '8460B5259601', 'distance_m': 746.9798466130279}, {'stop_id': '8460B5259701', 'distance_m': 577.5925690176058}, {'stop_id': '8460B5259801', 'distance_m': 769.7755631149972}, {'stop_id': '8460B5259901', 'distance_m': 773.5907430314251}]\n",
      "building_nearby_stops: [{'stop_id': '8460B5220101', 'distance_m': 370.26316200993006}, {'stop_id': '8460B5220201', 'distance_m': 678.7720388907327}, {'stop_id': '8460B522331', 'distance_m': 319.5179420353986}, {'stop_id': '8460B5243601', 'distance_m': 492.2775048504271}, {'stop_id': '8460B5222901', 'distance_m': 637.7801863304479}, {'stop_id': '8460B522311', 'distance_m': 368.63264148944114}, {'stop_id': '8460B523201', 'distance_m': 349.2963647509884}, {'stop_id': '8460B5232101', 'distance_m': 538.5302504932595}, {'stop_id': '8460B5245001', 'distance_m': 329.4004531886907}, {'stop_id': '8460B5225401', 'distance_m': 709.9859208173671}, {'stop_id': '8460B5225501', 'distance_m': 710.2057889152156}, {'stop_id': '8460B5225601', 'distance_m': 656.2203411417222}, {'stop_id': '8460B5225901', 'distance_m': 156.39453338681756}, {'stop_id': '8460B5226101', 'distance_m': 311.5329937661139}, {'stop_id': '8460B5226201', 'distance_m': 621.9661286408664}, {'stop_id': '8460B5226301', 'distance_m': 786.2075211034957}, {'stop_id': '8460B5231801', 'distance_m': 385.06488546009166}, {'stop_id': '8460B5225701', 'distance_m': 300.20638006553384}, {'stop_id': '8460B5230101', 'distance_m': 336.8361057776734}, {'stop_id': '8460B5230201', 'distance_m': 184.2935233707138}, {'stop_id': '8460B5230301', 'distance_m': 585.461953404642}, {'stop_id': '8460B5230401', 'distance_m': 721.4521536719417}, {'stop_id': '8460B5230501', 'distance_m': 785.6432395771378}, {'stop_id': '8460B5231901', 'distance_m': 553.6992774665647}, {'stop_id': '8460B5243701', 'distance_m': 332.71142884347785}, {'stop_id': '8460B5241201', 'distance_m': 286.5123537578056}, {'stop_id': '8460B5237101', 'distance_m': 337.19721718677584}, {'stop_id': '8460B5254101', 'distance_m': 344.8437298751218}, {'stop_id': '8460B525641', 'distance_m': 353.82837018287455}]\n",
      "place_nearby_stop_ids: ['84605257301', '8460B5255001', '8460B5255201', '8460B5255401', '8460B5256901', '8460B5257001', '8460B5257101', '8460B5257201', '8460B5259601', '8460B5259701', '8460B5259801', '8460B5259901']\n",
      "building_nearby_stop_ids: ['8460B5220101', '8460B5220201', '8460B522331', '8460B5243601', '8460B5222901', '8460B522311', '8460B523201', '8460B5232101', '8460B5245001', '8460B5225401', '8460B5225501', '8460B5225601', '8460B5225901', '8460B5226101', '8460B5226201', '8460B5226301', '8460B5231801', '8460B5225701', '8460B5230101', '8460B5230201', '8460B5230301', '8460B5230401', '8460B5230501', '8460B5231901', '8460B5243701', '8460B5241201', '8460B5237101', '8460B5254101', '8460B525641']\n"
     ]
    }
   ],
   "source": [
    "MAX_ACCESS_DISTANCE_METERS = 800\n",
    "\n",
    "if 'G' in locals() and G.number_of_nodes() > 0:\n",
    "    rahoon_node_id = \"Rahoon\"\n",
    "    portershed_node_id = \"Portershed a Dó\" # the building_name used as node ID\n",
    "\n",
    "    print(f\"--- Checking for exact node ID: '{rahoon_node_id}' ---\")\n",
    "    if G.has_node(rahoon_node_id):\n",
    "        print(f\"Node '{rahoon_node_id}' FOUND.\")\n",
    "        print(f\"Attributes: {G.nodes[rahoon_node_id]}\")\n",
    "    else:\n",
    "        print(f\"Node '{rahoon_node_id}' NOT FOUND by exact ID.\")\n",
    "\n",
    "    print(f\"\\n--- Checking for exact node ID: '{portershed_node_id}' ---\")\n",
    "    if G.has_node(portershed_node_id):\n",
    "        print(f\"Node '{portershed_node_id}' FOUND.\")\n",
    "        print(f\"Attributes: {G.nodes[portershed_node_id]}\")\n",
    "    else:\n",
    "        print(f\"Node '{portershed_node_id}' NOT FOUND by exact ID.\")\n",
    "else:\n",
    "    print(\"Graph G is not defined or is empty.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Step 3.1: Identify Relevant Stops (Nearby Stops) ---\n",
    "\n",
    "# Define the node IDs \n",
    "place_of_interest_rahoon = rahoon_node_id\n",
    "place_of_interest_portershed = portershed_node_id\n",
    "\n",
    "# Define the proximity threshold (user input)\n",
    "PROXIMITY_THRESHOLD_METERS = MAX_ACCESS_DISTANCE_METERS \n",
    "\n",
    "print(f\"Using proximity threshold: {PROXIMITY_THRESHOLD_METERS} meters.\")\n",
    "\n",
    "def get_nearby_stops(graph, poi_node_id, max_distance):\n",
    "    \"\"\"\n",
    "    Finds bus stops connected to a POI node via 'access_egress' edges\n",
    "    within a specified maximum distance, and returns their IDs and distances.\n",
    "    \"\"\"\n",
    "    nearby_stops_info = [] # list to store dictionaries\n",
    "\n",
    "    if not graph.has_node(poi_node_id):\n",
    "        print(f\"Warning: POI node '{poi_node_id}' not found in the graph.\")\n",
    "        return nearby_stops_info # Return empty list\n",
    "\n",
    "    # We are interested in edges FROM the POI TO a bus stop for \"access\"\n",
    "    # The graph stores bi-directional access/egress, so out_edges from POI is sufficient\n",
    "    # to find connected bus stops.\n",
    "    for u, v, data in graph.out_edges(poi_node_id, data=True):\n",
    "        edge_type = data.get('type')\n",
    "        edge_distance = data.get('distance_m', float('inf')) #  infinity if no distance\n",
    "\n",
    "        # Check if the edge is an access/egress edge\n",
    "        if edge_type == 'access_egress':\n",
    "            # Check if the connected node 'v' is a bus stop\n",
    "            if graph.has_node(v) and graph.nodes[v].get('type') == 'bus_stop':\n",
    "                # Check if the distance is within the threshold\n",
    "                if edge_distance <= max_distance:\n",
    "                    # Add a dictionary with stop_id and distance\n",
    "                    nearby_stops_info.append({'stop_id': v, 'distance_m': edge_distance}) \n",
    "            \n",
    "    return nearby_stops_info\n",
    "\n",
    "place_nearby_stops = get_nearby_stops(G, place_of_interest_rahoon, PROXIMITY_THRESHOLD_METERS)\n",
    "building_nearby_stops = get_nearby_stops(G, place_of_interest_portershed, PROXIMITY_THRESHOLD_METERS)\n",
    "place_nearby_stop_ids = [info['stop_id'] for info in place_nearby_stops]\n",
    "building_nearby_stop_ids = [info['stop_id'] for info in building_nearby_stops]\n",
    "print(f\"Proximity Threshold: {PROXIMITY_THRESHOLD_METERS} meters.\")\n",
    "print(f\"place_nearby_stops : {place_nearby_stops}\")\n",
    "print(f\"building_nearby_stops: {building_nearby_stops}\")\n",
    "print(f\"place_nearby_stop_ids: {place_nearby_stop_ids}\")\n",
    "print(f\"building_nearby_stop_ids: {building_nearby_stop_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Direct Transit Connection Analysis between Rahoon and Portershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin (Rahoon) Target Stops: ['84605257301', '8460B5255001', '8460B5255201', '8460B5255401', '8460B5256901', '8460B5257001', '8460B5257101', '8460B5257201', '8460B5259601', '8460B5259701', '8460B5259801', '8460B5259901']\n",
      "Destination (Portershed a Dó) Target Stops: ['8460B5220101', '8460B5220201', '8460B522331', '8460B5243601', '8460B5222901', '8460B522311', '8460B523201', '8460B5232101', '8460B5245001', '8460B5225401', '8460B5225501', '8460B5225601', '8460B5225901', '8460B5226101', '8460B5226201', '8460B5226301', '8460B5231801', '8460B5225701', '8460B5230101', '8460B5230201', '8460B5230301', '8460B5230401', '8460B5230501', '8460B5231901', '8460B5243701', '8460B5241201', '8460B5237101', '8460B5254101', '8460B525641']\n",
      "\n",
      "Analyzing routes from Rahoon area stops towards Portershed a Dó area stops...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_poi</th>\n",
       "      <th>destination_poi</th>\n",
       "      <th>route_id</th>\n",
       "      <th>origin_stop_id</th>\n",
       "      <th>destination_stop_id</th>\n",
       "      <th>origin_stop_order</th>\n",
       "      <th>destination_stop_order</th>\n",
       "      <th>hops</th>\n",
       "      <th>transit_distance_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5225601</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5847.477722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5230201</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>13479.613339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B522331</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13859.207580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5226201</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>14304.693633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5226301</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>14470.900167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B5225601</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5347.047644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B5230201</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>12979.183260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B522331</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>13358.777501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B5226201</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13804.263555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B5226301</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>13970.470088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B5225601</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1185.446039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B5230201</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8817.581655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B522331</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9197.175896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B5226201</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>9642.661950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B5226301</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>9808.868483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin_poi  destination_poi  route_id origin_stop_id destination_stop_id  \\\n",
       "6      Rahoon  Portershed a Dó       405   8460B5256901        8460B5225601   \n",
       "9      Rahoon  Portershed a Dó       405   8460B5256901        8460B5230201   \n",
       "5      Rahoon  Portershed a Dó       405   8460B5256901         8460B522331   \n",
       "7      Rahoon  Portershed a Dó       405   8460B5256901        8460B5226201   \n",
       "8      Rahoon  Portershed a Dó       405   8460B5256901        8460B5226301   \n",
       "11     Rahoon  Portershed a Dó       405   8460B5257001        8460B5225601   \n",
       "14     Rahoon  Portershed a Dó       405   8460B5257001        8460B5230201   \n",
       "10     Rahoon  Portershed a Dó       405   8460B5257001         8460B522331   \n",
       "12     Rahoon  Portershed a Dó       405   8460B5257001        8460B5226201   \n",
       "13     Rahoon  Portershed a Dó       405   8460B5257001        8460B5226301   \n",
       "1      Rahoon  Portershed a Dó       405    84605257301        8460B5225601   \n",
       "4      Rahoon  Portershed a Dó       405    84605257301        8460B5230201   \n",
       "0      Rahoon  Portershed a Dó       405    84605257301         8460B522331   \n",
       "2      Rahoon  Portershed a Dó       405    84605257301        8460B5226201   \n",
       "3      Rahoon  Portershed a Dó       405    84605257301        8460B5226301   \n",
       "\n",
       "    origin_stop_order  destination_stop_order  hops  transit_distance_m  \n",
       "6                   0                       9     9         5847.477722  \n",
       "9                   0                      11    11        13479.613339  \n",
       "5                   0                      12    12        13859.207580  \n",
       "7                   0                      13    13        14304.693633  \n",
       "8                   0                      16    16        14470.900167  \n",
       "11                  1                       9     8         5347.047644  \n",
       "14                  1                      11    10        12979.183260  \n",
       "10                  1                      12    11        13358.777501  \n",
       "12                  1                      13    12        13804.263555  \n",
       "13                  1                      16    15        13970.470088  \n",
       "1                   7                       9     2         1185.446039  \n",
       "4                   7                      11     4         8817.581655  \n",
       "0                   7                      12     5         9197.175896  \n",
       "2                   7                      13     6         9642.661950  \n",
       "3                   7                      16     9         9808.868483  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- place_to_building_connections_df with updated total journey distance (Origin Walk + Transit + Destination Walk) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_poi</th>\n",
       "      <th>destination_poi</th>\n",
       "      <th>route_id</th>\n",
       "      <th>origin_stop_id</th>\n",
       "      <th>destination_stop_id</th>\n",
       "      <th>origin_stop_order</th>\n",
       "      <th>destination_stop_order</th>\n",
       "      <th>hops</th>\n",
       "      <th>transit_distance_m</th>\n",
       "      <th>walking_distance_from_origin_poi_m</th>\n",
       "      <th>walking_distance_to_dest_poi_m</th>\n",
       "      <th>total_journey_distance_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B5225601</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1185.446039</td>\n",
       "      <td>729.996903</td>\n",
       "      <td>656.220341</td>\n",
       "      <td>2571.663283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B5225601</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5347.047644</td>\n",
       "      <td>767.372702</td>\n",
       "      <td>656.220341</td>\n",
       "      <td>6770.640687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5225601</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5847.477722</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>656.220341</td>\n",
       "      <td>7230.160757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B5230201</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8817.581655</td>\n",
       "      <td>729.996903</td>\n",
       "      <td>184.293523</td>\n",
       "      <td>9731.872082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B522331</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9197.175896</td>\n",
       "      <td>729.996903</td>\n",
       "      <td>319.517942</td>\n",
       "      <td>10246.690741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B5226201</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>9642.661950</td>\n",
       "      <td>729.996903</td>\n",
       "      <td>621.966129</td>\n",
       "      <td>10994.624981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>84605257301</td>\n",
       "      <td>8460B5226301</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>9808.868483</td>\n",
       "      <td>729.996903</td>\n",
       "      <td>786.207521</td>\n",
       "      <td>11325.072908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B5230201</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>12979.183260</td>\n",
       "      <td>767.372702</td>\n",
       "      <td>184.293523</td>\n",
       "      <td>13930.849485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5230201</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>13479.613339</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>184.293523</td>\n",
       "      <td>14390.369556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B522331</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>13358.777501</td>\n",
       "      <td>767.372702</td>\n",
       "      <td>319.517942</td>\n",
       "      <td>14445.668145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B522331</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13859.207580</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>319.517942</td>\n",
       "      <td>14905.188215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B5226201</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13804.263555</td>\n",
       "      <td>767.372702</td>\n",
       "      <td>621.966129</td>\n",
       "      <td>15193.602385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5257001</td>\n",
       "      <td>8460B5226301</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>13970.470088</td>\n",
       "      <td>767.372702</td>\n",
       "      <td>786.207521</td>\n",
       "      <td>15524.050311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5226201</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>14304.693633</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>621.966129</td>\n",
       "      <td>15653.122456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5226301</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>14470.900167</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>786.207521</td>\n",
       "      <td>15983.570382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin_poi  destination_poi  route_id origin_stop_id destination_stop_id  \\\n",
       "1      Rahoon  Portershed a Dó       405    84605257301        8460B5225601   \n",
       "11     Rahoon  Portershed a Dó       405   8460B5257001        8460B5225601   \n",
       "6      Rahoon  Portershed a Dó       405   8460B5256901        8460B5225601   \n",
       "4      Rahoon  Portershed a Dó       405    84605257301        8460B5230201   \n",
       "0      Rahoon  Portershed a Dó       405    84605257301         8460B522331   \n",
       "2      Rahoon  Portershed a Dó       405    84605257301        8460B5226201   \n",
       "3      Rahoon  Portershed a Dó       405    84605257301        8460B5226301   \n",
       "14     Rahoon  Portershed a Dó       405   8460B5257001        8460B5230201   \n",
       "9      Rahoon  Portershed a Dó       405   8460B5256901        8460B5230201   \n",
       "10     Rahoon  Portershed a Dó       405   8460B5257001         8460B522331   \n",
       "5      Rahoon  Portershed a Dó       405   8460B5256901         8460B522331   \n",
       "12     Rahoon  Portershed a Dó       405   8460B5257001        8460B5226201   \n",
       "13     Rahoon  Portershed a Dó       405   8460B5257001        8460B5226301   \n",
       "7      Rahoon  Portershed a Dó       405   8460B5256901        8460B5226201   \n",
       "8      Rahoon  Portershed a Dó       405   8460B5256901        8460B5226301   \n",
       "\n",
       "    origin_stop_order  destination_stop_order  hops  transit_distance_m  \\\n",
       "1                   7                       9     2         1185.446039   \n",
       "11                  1                       9     8         5347.047644   \n",
       "6                   0                       9     9         5847.477722   \n",
       "4                   7                      11     4         8817.581655   \n",
       "0                   7                      12     5         9197.175896   \n",
       "2                   7                      13     6         9642.661950   \n",
       "3                   7                      16     9         9808.868483   \n",
       "14                  1                      11    10        12979.183260   \n",
       "9                   0                      11    11        13479.613339   \n",
       "10                  1                      12    11        13358.777501   \n",
       "5                   0                      12    12        13859.207580   \n",
       "12                  1                      13    12        13804.263555   \n",
       "13                  1                      16    15        13970.470088   \n",
       "7                   0                      13    13        14304.693633   \n",
       "8                   0                      16    16        14470.900167   \n",
       "\n",
       "    walking_distance_from_origin_poi_m  walking_distance_to_dest_poi_m  \\\n",
       "1                           729.996903                      656.220341   \n",
       "11                          767.372702                      656.220341   \n",
       "6                           726.462693                      656.220341   \n",
       "4                           729.996903                      184.293523   \n",
       "0                           729.996903                      319.517942   \n",
       "2                           729.996903                      621.966129   \n",
       "3                           729.996903                      786.207521   \n",
       "14                          767.372702                      184.293523   \n",
       "9                           726.462693                      184.293523   \n",
       "10                          767.372702                      319.517942   \n",
       "5                           726.462693                      319.517942   \n",
       "12                          767.372702                      621.966129   \n",
       "13                          767.372702                      786.207521   \n",
       "7                           726.462693                      621.966129   \n",
       "8                           726.462693                      786.207521   \n",
       "\n",
       "    total_journey_distance_m  \n",
       "1                2571.663283  \n",
       "11               6770.640687  \n",
       "6                7230.160757  \n",
       "4                9731.872082  \n",
       "0               10246.690741  \n",
       "2               10994.624981  \n",
       "3               11325.072908  \n",
       "14              13930.849485  \n",
       "9               14390.369556  \n",
       "10              14445.668145  \n",
       "5               14905.188215  \n",
       "12              15193.602385  \n",
       "13              15524.050311  \n",
       "7               15653.122456  \n",
       "8               15983.570382  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    display = print # Fallback to simple print if not in IPython\n",
    "\n",
    "def direct_transit_conn_between_places(G, bus_timetables, origin_poi_name, \n",
    "                                       origin_nearby_stops_info, destination_poi_name, \n",
    "                                       destination_nearby_stops_info):\n",
    "    \"\"\"\n",
    "    Analyzes direct public transit connections between two sets of nearby stops \n",
    "    for given points of interests (POIs).\n",
    "\n",
    "    Args:\n",
    "        G (nx.DiGraph): The NetworkX graph containing transit network data. \n",
    "                        Edges should have 'type' ('transit'), 'route_id', and 'distance_m'.\n",
    "        bus_timetables (pd.DataFrame): DataFrame with bus timetable information, including\n",
    "                                       'route_id', 'stop_id_mapped', and 'stop_order_on_route'.\n",
    "        origin_poi_name (str): Name of the origin POI (e.g., \"Rahoon\").\n",
    "        origin_nearby_stops_info (list): List of dictionaries for stops near the origin POI.\n",
    "                                         Each dict: {'stop_id': str, 'distance_m': float}\n",
    "        destination_poi_name (str): Name of the destination POI (e.g., \"Portershed\").\n",
    "        destination_nearby_stops_info (list): List of dictionaries for stops near the destination POI.\n",
    "                                              Each dict: {'stop_id': str, 'distance_m': float}\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two DataFrames:\n",
    "               - connections_df (pd.DataFrame): DataFrame of direct transit connections found.\n",
    "               - no_connection_df (pd.DataFrame): DataFrame of routes serving origin stops \n",
    "                                                  but not connecting to destination stops in sequence.\n",
    "               Returns (None, None) if critical input errors occur.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Initialization and Input Preparation ---\n",
    "    if not origin_nearby_stops_info or not destination_nearby_stops_info:\n",
    "        print(f\"Warning: Input list 'origin_nearby_stops_info' or 'destination_nearby_stops_info' is empty.\")\n",
    "\n",
    "    origin_target_stop_ids = [info['stop_id'] for info in origin_nearby_stops_info]\n",
    "    destination_target_stop_ids = [info['stop_id'] for info in destination_nearby_stops_info]\n",
    "\n",
    "    print(f\"Origin ({origin_poi_name}) Target Stops: {origin_target_stop_ids}\")\n",
    "    print(f\"Destination ({destination_poi_name}) Target Stops: {destination_target_stop_ids}\")\n",
    "\n",
    "    direct_transit_connections = []\n",
    "    origin_routes_no_destination_connection = []\n",
    "\n",
    "    # --- Main Analysis Logic ---\n",
    "    required_cols = ['route_id', 'stop_id_mapped', 'stop_order_on_route']\n",
    "    if not isinstance(bus_timetables, pd.DataFrame) or not all(col in bus_timetables.columns for col in required_cols):\n",
    "        print(f\"Error: 'bus_timetables' DataFrame is not valid or is missing required columns: {required_cols}\")\n",
    "        return None, None \n",
    "    if not hasattr(G, 'edges'):\n",
    "        print(\"Error: NetworkX graph 'G' is not valid.\")\n",
    "        return None, None\n",
    "    if not origin_target_stop_ids or not destination_target_stop_ids:\n",
    "        print(f\"Error: Either Origin ({origin_poi_name}) or Destination ({destination_poi_name}) target stop lists are empty. Cannot proceed.\")\n",
    "        # Return empty DataFrames as per function definition\n",
    "        return pd.DataFrame(columns=['origin_poi', 'destination_poi', 'route_id', 'origin_stop_id', \n",
    "                                     'destination_stop_id', 'origin_stop_order', 'destination_stop_order', \n",
    "                                     'hops', 'transit_distance_m']), \\\n",
    "               pd.DataFrame(columns=['origin_stop_id', 'route_id', 'message'])\n",
    "\n",
    "\n",
    "    print(f\"\\nAnalyzing routes from {origin_poi_name} area stops towards {destination_poi_name} area stops...\")\n",
    "\n",
    "    for r_stop_id in origin_target_stop_ids:\n",
    "        routes_serving_r_stop_df = bus_timetables[bus_timetables['stop_id_mapped'] == r_stop_id]\n",
    "        \n",
    "        if routes_serving_r_stop_df.empty:\n",
    "            continue\n",
    "\n",
    "        unique_routes_for_this_r_stop = routes_serving_r_stop_df['route_id'].unique()\n",
    "\n",
    "        for route_id_val in unique_routes_for_this_r_stop:\n",
    "            route_sequence_df = bus_timetables[bus_timetables['route_id'] == route_id_val].sort_values(by='stop_order_on_route')\n",
    "            \n",
    "            if route_sequence_df.empty: \n",
    "                continue \n",
    "\n",
    "            stop_to_order_map = pd.Series(route_sequence_df['stop_order_on_route'].values, index=route_sequence_df['stop_id_mapped']).to_dict()\n",
    "            \n",
    "            if r_stop_id not in stop_to_order_map:\n",
    "                continue\n",
    "            r_stop_order = stop_to_order_map[r_stop_id]\n",
    "            \n",
    "            found_connection_on_this_route_for_r_stop = False\n",
    "\n",
    "            for p_stop_id in destination_target_stop_ids:\n",
    "                if p_stop_id in stop_to_order_map:\n",
    "                    p_stop_order = stop_to_order_map[p_stop_id]\n",
    "                    \n",
    "                    if r_stop_order < p_stop_order:\n",
    "                        found_connection_on_this_route_for_r_stop = True\n",
    "                        hops = p_stop_order - r_stop_order\n",
    "                        current_distance_m = 0.0\n",
    "                        path_found_in_graph = True\n",
    "                        \n",
    "                        path_segment_df = route_sequence_df[\n",
    "                            (route_sequence_df['stop_order_on_route'] >= r_stop_order) &\n",
    "                            (route_sequence_df['stop_order_on_route'] <= p_stop_order)\n",
    "                        ]\n",
    "                        actual_stops_in_path_sequence = path_segment_df['stop_id_mapped'].tolist()\n",
    "                        \n",
    "                        if len(actual_stops_in_path_sequence) < 2:\n",
    "                            if r_stop_id == p_stop_id:\n",
    "                                current_distance_m = 0.0\n",
    "                            else: \n",
    "                                path_found_in_graph = False\n",
    "                        else:\n",
    "                            for i in range(len(actual_stops_in_path_sequence) - 1):\n",
    "                                from_s = actual_stops_in_path_sequence[i]\n",
    "                                to_s = actual_stops_in_path_sequence[i+1]\n",
    "                                \n",
    "                                if G.has_edge(from_s, to_s):\n",
    "                                    edge_data = G.get_edge_data(from_s, to_s)\n",
    "                                    if edge_data.get('type') == 'transit' and edge_data.get('route_id') == route_id_val:\n",
    "                                        current_distance_m += edge_data.get('distance_m', 0.0)\n",
    "                                    else:\n",
    "                                        path_found_in_graph = False; break\n",
    "                                else:\n",
    "                                    path_found_in_graph = False; break\n",
    "                        \n",
    "                        if not path_found_in_graph:\n",
    "                            current_distance_m = None \n",
    "                            \n",
    "                        connection_details = {\n",
    "                            'origin_poi': origin_poi_name, \n",
    "                            'destination_poi': destination_poi_name,\n",
    "                            'route_id': route_id_val,\n",
    "                            'origin_stop_id': r_stop_id,\n",
    "                            'destination_stop_id': p_stop_id,\n",
    "                            'origin_stop_order': r_stop_order,\n",
    "                            'destination_stop_order': p_stop_order,\n",
    "                            'hops': hops,\n",
    "                            'transit_distance_m': current_distance_m \n",
    "                        }\n",
    "                        direct_transit_connections.append(connection_details)\n",
    "\n",
    "            if not found_connection_on_this_route_for_r_stop and r_stop_id in stop_to_order_map :\n",
    "                 origin_routes_no_destination_connection.append({\n",
    "                     'origin_stop_id': r_stop_id,\n",
    "                     'route_id': route_id_val,\n",
    "                     'message': f\"Route {route_id_val} serves {origin_poi_name} area stop {r_stop_id} but does not connect to any target {destination_poi_name} area stops in sequence.\"\n",
    "                 })\n",
    "\n",
    "    # --- Prepare Output DataFrames ---\n",
    "    connections_df = pd.DataFrame()\n",
    "    if direct_transit_connections:\n",
    "        connections_df = pd.DataFrame(direct_transit_connections)\n",
    "        connections_df = connections_df.sort_values(by=['route_id', 'origin_stop_order', 'hops'])\n",
    "\n",
    "    no_connection_df = pd.DataFrame()\n",
    "    if origin_routes_no_destination_connection:\n",
    "        no_connection_df = pd.DataFrame(origin_routes_no_destination_connection).drop_duplicates()\n",
    "        \n",
    "    return connections_df, no_connection_df\n",
    "\n",
    "\n",
    "\n",
    "place_to_building_connections_df, place_to_building_no_connection_df = direct_transit_conn_between_places(G, bus_timetables, place_of_interest_rahoon, place_nearby_stops, \n",
    "                                   place_of_interest_portershed, building_nearby_stops)\n",
    "display(place_to_building_connections_df)\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "# --- Calculate Walking Distance from Origin POI (Rahoon) to its nearby bus stops ---\n",
    "unique_origin_stops = place_to_building_connections_df['origin_stop_id'].unique()\n",
    "origin_walking_distances_map = {}\n",
    "\n",
    "for stop_id in unique_origin_stops:\n",
    "    walking_distance = np.nan\n",
    "    if G.has_node(stop_id):\n",
    "        # Check for a direct access_egress edge FROM the Rahoon POI node TO the origin bus stop\n",
    "        if G.has_edge(rahoon_node_id, stop_id):\n",
    "            edge_data = G.get_edge_data(rahoon_node_id, stop_id)\n",
    "            if edge_data.get('type') == 'access_egress':\n",
    "                walking_distance = edge_data.get('distance_m', np.nan)\n",
    "    else:\n",
    "        print(f\"Warning: Origin stop ID '{stop_id}' from place_to_building_connections_df not found in graph G.\")\n",
    "    origin_walking_distances_map[stop_id] = walking_distance\n",
    "\n",
    "place_to_building_connections_df['walking_distance_from_origin_poi_m'] = place_to_building_connections_df['origin_stop_id'].map(origin_walking_distances_map)\n",
    "\n",
    "# --- Calculate Walking Distance from Building to its nearby bus stops\n",
    "unique_dest_stops = place_to_building_connections_df['destination_stop_id'].unique()\n",
    "dest_walking_distances_map = {}  # To store {dest_stop_id: walking_distance_m}\n",
    "\n",
    "for stop_id in unique_dest_stops:\n",
    "    walking_distance = np.nan # Default to NaN if no direct walking edge found\n",
    "    if G.has_node(stop_id): # \n",
    "        # Check for a direct access_egress edge from the bus stop TO the Portershed building node\n",
    "        if G.has_edge(stop_id, portershed_node_id):\n",
    "            edge_data = G.get_edge_data(stop_id, portershed_node_id)\n",
    "            if edge_data.get('type') == 'access_egress': \n",
    "                walking_distance = edge_data.get('distance_m', np.nan)\n",
    "    else:\n",
    "        print(f\"Warning: Destination stop ID '{stop_id}' from connections_df not found in graph G.\")\n",
    "        \n",
    "    dest_walking_distances_map[stop_id] = walking_distance\n",
    "place_to_building_connections_df['walking_distance_to_dest_poi_m'] = place_to_building_connections_df['destination_stop_id'].map(dest_walking_distances_map)\n",
    "\n",
    "# create a total travel cost (transit_distance + walking_distance)\n",
    "if 'transit_distance_m' in place_to_building_connections_df.columns:\n",
    "    # Convert to numeric\n",
    "    place_to_building_connections_df['numeric_origin_walk_dist'] = pd.to_numeric(place_to_building_connections_df['walking_distance_from_origin_poi_m'], errors='coerce')\n",
    "    place_to_building_connections_df['numeric_transit_dist'] = pd.to_numeric(place_to_building_connections_df['transit_distance_m'], errors='coerce')\n",
    "    place_to_building_connections_df['numeric_dest_walk_dist'] = pd.to_numeric(place_to_building_connections_df['walking_distance_to_dest_poi_m'], errors='coerce')\n",
    "\n",
    "    \n",
    "    # Calculate total distance only if all three components are available\n",
    "    place_to_building_connections_df['total_journey_distance_m'] = place_to_building_connections_df[\n",
    "        ['numeric_origin_walk_dist', 'numeric_transit_dist', 'numeric_dest_walk_dist']\n",
    "    ].sum(axis=1, min_count=3) # min_count=3 ensures all parts are present \n",
    "        \n",
    "    print(\"\\n--- place_to_building_connections_df with updated total journey distance (Origin Walk + Transit + Destination Walk) ---\")\n",
    "    place_to_building_connections_df = place_to_building_connections_df.drop(columns=['numeric_origin_walk_dist', 'numeric_transit_dist', 'numeric_dest_walk_dist'])\n",
    "    display(place_to_building_connections_df.sort_values(by='total_journey_distance_m'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Public Transport Accessibility Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using beta (decay parameter): 0.0001\n",
      "\u001b[38;5;27m\u001b[1m\n",
      "Accessibility Score between Rahoon and Portershed a Dó is: 63.61\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_exp_decay_accessibility_score(connections_df,\n",
    "                                            distance_column='total_journey_distance_m',\n",
    "                                            beta=0.0001, \n",
    "                                            weights={'d1': 0.50, 'd2': 0.30, 'd3': 0.20}):\n",
    "    \"\"\"\n",
    "    Calculates an accessibility score (0-100, higher is better) based on exponential\n",
    "    decay of the top three unique shortest journey distances, with given weights.\n",
    "\n",
    "    Args:\n",
    "        connections_df (pd.DataFrame): DataFrame containing journey data.\n",
    "        distance_column (str): Name of the column with total journey distances.\n",
    "        beta (float): The decay parameter for the exponential function.\n",
    "                      Adjust based on distance units and desired sensitivity.\n",
    "                      A common way to set beta is beta = ln(2) / d_half,\n",
    "                      where d_half is the distance at which accessibility is halved.\n",
    "                      (ln(2) is approx 0.693)\n",
    "        weights (dict): Dictionary of weights for d1, d2, d3.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated accessibility score (0-100), or 0.0 if no valid distances.\n",
    "    \"\"\"\n",
    "    if distance_column not in connections_df.columns:\n",
    "        print(f\"Error: Distance column '{distance_column}' not found in DataFrame.\")\n",
    "        return 0.0  \n",
    "\n",
    "    valid_distances_df = connections_df.dropna(subset=[distance_column]).copy()\n",
    "    if valid_distances_df.empty:\n",
    "        print(\"No valid journey distances available to calculate score.\")\n",
    "        return 0.0\n",
    "\n",
    "    valid_distances_df = valid_distances_df.sort_values(by=distance_column)\n",
    "    unique_shortest_distances = valid_distances_df[distance_column].unique()\n",
    "    \n",
    "    if len(unique_shortest_distances) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    distances_to_score = {'d1': np.nan, 'd2': np.nan, 'd3': np.nan}\n",
    "    if len(unique_shortest_distances) >= 1: distances_to_score['d1'] = unique_shortest_distances[0]\n",
    "    if len(unique_shortest_distances) >= 2: distances_to_score['d2'] = unique_shortest_distances[1]\n",
    "    if len(unique_shortest_distances) >= 3: distances_to_score['d3'] = unique_shortest_distances[2]\n",
    "\n",
    "    print(f\"Using beta (decay parameter): {beta}\")\n",
    "   \n",
    "\n",
    "    score_components = {}\n",
    "    for key, d_val in distances_to_score.items():\n",
    "        if not np.isnan(d_val) and d_val >= 0: # Ensure distance is non-negative\n",
    "            component_score = 100 * np.exp(-beta * d_val)\n",
    "            score_components[key] = component_score\n",
    "        else:\n",
    "            score_components[key] = 0.0 # No contribution or invalid distance\n",
    "\n",
    "\n",
    "    final_accessibility_score = (weights.get('d1', 0) * score_components.get('d1', 0.0)) + \\\n",
    "                                (weights.get('d2', 0) * score_components.get('d2', 0.0)) + \\\n",
    "                                (weights.get('d3', 0) * score_components.get('d3', 0.0))\n",
    "    \n",
    "    return final_accessibility_score\n",
    "\n",
    "\n",
    "# Calculate accessibility score\n",
    "accessibility_score = round(calculate_exp_decay_accessibility_score(place_to_building_connections_df,\n",
    "                                            distance_column='total_journey_distance_m',\n",
    "                                            beta=0.0001, \n",
    "                                            weights={'d1': 0.50, 'd2': 0.30, 'd3': 0.20}),2)\n",
    "\n",
    "# Get unique origin and destination from the connections dataframe\n",
    "origin = place_to_building_connections_df['origin_poi'].iloc[0]\n",
    "destination = place_to_building_connections_df['destination_poi'].iloc[0]\n",
    "\n",
    "\n",
    "if accessibility_score >= 80:\n",
    "    color = '\\033[38;5;22m'   # Dark green for high accessibility\n",
    "elif accessibility_score >= 60:\n",
    "    color = '\\033[38;5;27m'   # Blue for moderate accessibility\n",
    "elif accessibility_score >= 30:\n",
    "    color = '\\033[38;5;214m'   # Orange for medium-low accessibility\n",
    "else:\n",
    "    color = '\\033[38;5;196m'   # Red for low accessibility\n",
    "\n",
    "print(f\"{color}\\033[1m\\nAccessibility Score between {origin} and {destination} is: {accessibility_score}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the artifacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Define file paths for all relevant artifacts\n",
    "# places_summary_path = os.path.join(artifact_dir, \"galway_places_summary_df1.csv\")\n",
    "# buildings_summary_path = os.path.join(artifact_dir, \"galway_buildings_summary_df1.csv\")\n",
    "# bus_stops_path = os.path.join(artifact_dir, \"bus_stops_gdf.csv\")\n",
    "# bus_timetables_path = os.path.join(artifact_dir, \"bus_timetables.csv\")\n",
    "# bus_routes_path = os.path.join(artifact_dir, \"bus_routes_gdf.csv\")\n",
    "# graph_pickle_path = os.path.join(artifact_dir, \"galway_transport_graph.gpickle\")\n",
    "# place_to_building_connections_path = os.path.join(artifact_dir, \"place_to_building_connections.csv\")\n",
    "\n",
    "# # Save the loaded dataframes and graph back to artifact_dir for reproducibility\n",
    "\n",
    "# if os.path.isdir(artifact_dir):\n",
    "#     # Save galway_places_summary_df1\n",
    "#     if galway_places_summary_df1 is not None:\n",
    "#         galway_places_summary_df1.to_csv(places_summary_path)\n",
    "#     # Save galway_buildings_summary_df1\n",
    "#     if galway_buildings_summary_df1 is not None:\n",
    "#         galway_buildings_summary_df1.to_csv(buildings_summary_path)\n",
    "#     # Save bus_stops_gdf\n",
    "#     if bus_stops_gdf is not None:\n",
    "#         bus_stops_gdf.to_csv(bus_stops_path)\n",
    "#     # Save bus_timetables\n",
    "#     if bus_timetables is not None:\n",
    "#         bus_timetables.to_csv(bus_timetables_path)\n",
    "#     # Save bus_routes_gdf\n",
    "#     if bus_routes_gdf is not None:\n",
    "#         bus_routes_gdf.to_csv(bus_routes_path)\n",
    "#     # Save the graph G as a pickle file\n",
    "#     if 'G' in locals() and G is not None:\n",
    "#         with open(graph_pickle_path, \"wb\") as f:\n",
    "#             pickle.dump(G, f, pickle.HIGHEST_PROTOCOL)\n",
    "#     print(\"Artifacts saved successfully.\")\n",
    "\n",
    "#     if place_to_building_connections_df is not None:\n",
    "#         place_to_building_connections_df.to_csv(place_to_building_connections_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the artifacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for all relevant artifacts\n",
    "places_summary_path = os.path.join(artifact_dir, \"galway_places_summary_df1.csv\")\n",
    "buildings_summary_path = os.path.join(artifact_dir, \"galway_buildings_summary_df1.csv\")\n",
    "bus_stops_path = os.path.join(artifact_dir, \"bus_stops_gdf.csv\")\n",
    "bus_timetables_path = os.path.join(artifact_dir, \"bus_timetables.csv\")\n",
    "bus_routes_path = os.path.join(artifact_dir, \"bus_routes_gdf.csv\")\n",
    "graph_pickle_path = os.path.join(artifact_dir, \"galway_transport_graph.gpickle\")\n",
    "place_to_building_connections_path = os.path.join(artifact_dir, \"place_to_building_connections.csv\")\n",
    "\n",
    "# Load the dataframes and graph from artifact_dir if available\n",
    "if os.path.isdir(artifact_dir):\n",
    "\n",
    "    # Read places summary, preserving index if present\n",
    "    if os.path.exists(places_summary_path):\n",
    "        galway_places_summary_df1 = pd.read_csv(places_summary_path, index_col=0)\n",
    "    else:\n",
    "        galway_places_summary_df1 = None\n",
    "\n",
    "    # Read buildings summary, preserving index (e.g., B1, B2, ...)\n",
    "    if os.path.exists(buildings_summary_path):\n",
    "        galway_buildings_summary_df1 = pd.read_csv(buildings_summary_path, index_col=0)\n",
    "    else:\n",
    "        galway_buildings_summary_df1 = None\n",
    "\n",
    "    # Read bus stops, preserving index if present\n",
    "    if os.path.exists(bus_stops_path):\n",
    "        bus_stops_gdf = pd.read_csv(bus_stops_path, index_col=0)\n",
    "    else:\n",
    "        bus_stops_gdf = None\n",
    "\n",
    "    # Read bus timetables, preserving index if present\n",
    "    if os.path.exists(bus_timetables_path):\n",
    "        bus_timetables = pd.read_csv(bus_timetables_path, index_col=0)\n",
    "    else:\n",
    "        bus_timetables = None\n",
    "\n",
    "    # Read bus routes, preserving index (e.g., BR1, BR2, ...)\n",
    "    if os.path.exists(bus_routes_path):\n",
    "        bus_routes_gdf = pd.read_csv(bus_routes_path, index_col=0)\n",
    "    else:\n",
    "        bus_routes_gdf = None\n",
    "\n",
    "    # Load the graph G from pickle if available\n",
    "    if os.path.exists(graph_pickle_path):\n",
    "        with open(graph_pickle_path, \"rb\") as f:\n",
    "            G = pickle.load(f)\n",
    "    else:\n",
    "        G = None\n",
    "    print(\"Artifacts loaded successfully.\")\n",
    "else:\n",
    "    all_timetables_df = None\n",
    "    galway_places_summary_df1 = None\n",
    "    galway_buildings_summary_df1 = None\n",
    "    bus_stops_gdf = None\n",
    "    bus_timetables = None\n",
    "    bus_routes_gdf = None\n",
    "    G = None\n",
    "    print(\"No artifact directory found. Dataframes and graph not loaded.\")\n",
    "\n",
    "if os.path.isdir(artifact_dir):\n",
    "    if os.path.exists(place_to_building_connections_path):\n",
    "        place_to_building_connections_df = pd.read_csv(place_to_building_connections_path, index_col=0)\n",
    "    else:\n",
    "        place_to_building_connections_df = None\n",
    "else:\n",
    "    place_to_building_connections_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph G loaded: 888 nodes, 25733 edges\n",
      "DataFrame place_to_building_connections_df loaded: (15, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_poi</th>\n",
       "      <th>destination_poi</th>\n",
       "      <th>route_id</th>\n",
       "      <th>origin_stop_id</th>\n",
       "      <th>destination_stop_id</th>\n",
       "      <th>origin_stop_order</th>\n",
       "      <th>destination_stop_order</th>\n",
       "      <th>hops</th>\n",
       "      <th>transit_distance_m</th>\n",
       "      <th>walking_distance_from_origin_poi_m</th>\n",
       "      <th>walking_distance_to_dest_poi_m</th>\n",
       "      <th>total_journey_distance_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5225601</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5847.477722</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>656.220341</td>\n",
       "      <td>7230.160757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5230201</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>13479.613339</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>184.293523</td>\n",
       "      <td>14390.369556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B522331</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13859.207580</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>319.517942</td>\n",
       "      <td>14905.188215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5226201</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>14304.693633</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>621.966129</td>\n",
       "      <td>15653.122456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rahoon</td>\n",
       "      <td>Portershed a Dó</td>\n",
       "      <td>405</td>\n",
       "      <td>8460B5256901</td>\n",
       "      <td>8460B5226301</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>14470.900167</td>\n",
       "      <td>726.462693</td>\n",
       "      <td>786.207521</td>\n",
       "      <td>15983.570382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  origin_poi  destination_poi  route_id origin_stop_id destination_stop_id  \\\n",
       "6     Rahoon  Portershed a Dó       405   8460B5256901        8460B5225601   \n",
       "9     Rahoon  Portershed a Dó       405   8460B5256901        8460B5230201   \n",
       "5     Rahoon  Portershed a Dó       405   8460B5256901         8460B522331   \n",
       "7     Rahoon  Portershed a Dó       405   8460B5256901        8460B5226201   \n",
       "8     Rahoon  Portershed a Dó       405   8460B5256901        8460B5226301   \n",
       "\n",
       "   origin_stop_order  destination_stop_order  hops  transit_distance_m  \\\n",
       "6                  0                       9     9         5847.477722   \n",
       "9                  0                      11    11        13479.613339   \n",
       "5                  0                      12    12        13859.207580   \n",
       "7                  0                      13    13        14304.693633   \n",
       "8                  0                      16    16        14470.900167   \n",
       "\n",
       "   walking_distance_from_origin_poi_m  walking_distance_to_dest_poi_m  \\\n",
       "6                          726.462693                      656.220341   \n",
       "9                          726.462693                      184.293523   \n",
       "5                          726.462693                      319.517942   \n",
       "7                          726.462693                      621.966129   \n",
       "8                          726.462693                      786.207521   \n",
       "\n",
       "   total_journey_distance_m  \n",
       "6               7230.160757  \n",
       "9              14390.369556  \n",
       "5              14905.188215  \n",
       "7              15653.122456  \n",
       "8              15983.570382  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Graph G loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"DataFrame place_to_building_connections_df loaded: {place_to_building_connections_df.shape}\")\n",
    "display(place_to_building_connections_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Langchain\n",
    "# !pip uninstall numpy\n",
    "\n",
    "# ! pip install langchain openai pandas tabulate\n",
    "# ! pip install langchain-ollama\n",
    "\n",
    "# !pip install numpy==1.21.6\n",
    "\n",
    "# connections_df= place_to_building_connections_df\n",
    "\n",
    "# journey_df_filtered = connections_df[(connections_df['origin_stop_id'].astype(str) == '8460B5256901') & (connections_df['destination_stop_id'].astype(str) == '8460B5225601')].copy()\n",
    "\n",
    "# journey_df_filtered\n",
    "\n",
    "# connections_df= place_to_building_connections_df\n",
    "\n",
    "# journey_df_filtered = connections_df[(connections_df['origin_stop_id'].astype(str) == '8460B5256901') & (connections_df['destination_stop_id'].astype(str) == '8460B5225601')].copy()\n",
    "\n",
    "# journey_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model configuration 1\n",
    "llm = ChatOllama(model=\"llama3\") \n",
    "# print(\"ChatOllama initialized successfully with the new package.\")\n",
    "from langchain.tools import tool\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent \n",
    "from pydantic import BaseModel, Field\n",
    "# !pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_journey_accessibility_info_v2(\n",
    "#     origin_poi_name: str, \n",
    "#     destination_poi_name: str,\n",
    "#     connections_df: pd.DataFrame # Pass the specific DataFrame as an argument\n",
    "# ) -> str:\n",
    "#     \"\"\"\n",
    "#     Provides accessibility information for a journey between a specified origin POI \n",
    "#     and destination POI using the provided connections_df. \n",
    "#     It dynamically calculates the accessibility score using \n",
    "#     'calculate_exp_decay_accessibility_score' and details the top journey option(s).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if connections_df is None or connections_df.empty:\n",
    "#         return \"Error: Input connections_df is not provided or is empty.\"\n",
    "\n",
    "#     # Filter the DataFrame for the specific journey\n",
    "\n",
    "#     # confirm the data is for the requested pair.\n",
    "#     journey_df_filtered = connections_df[\n",
    "#         (connections_df['origin_poi'].astype(str).str.lower() == origin_poi_name.lower()) &\n",
    "#         (connections_df['destination_poi'].astype(str).str.lower() == destination_poi_name.lower())\n",
    "#     ].copy() # Use .copy() to avoid SettingWithCopyWarning if you modify it later\n",
    "\n",
    "#     if journey_df_filtered.empty:\n",
    "#         return (f\"No journey data found in the provided connections_df for \"\n",
    "#                 f\"{origin_poi_name} to {destination_poi_name}.\")\n",
    "\n",
    "#     # Calculate the accessibility score using the function\n",
    "#     # Make sure 'calculate_exp_decay_accessibility_score' function is defined and accessible\n",
    "#     try:\n",
    "#         # Pass the filtered DataFrame specific to this O-D pair for score calculation\n",
    "#         accessibility_score = calculate_exp_decay_accessibility_score(journey_df_filtered) \n",
    "#     except Exception as e:\n",
    "#         return f\"Error calculating accessibility score: {e}. Make sure 'calculate_exp_decay_accessibility_score' is correctly defined and accessible.\"\n",
    "\n",
    "#     # Sort by total journey distance to find the best options\n",
    "#     top_options_df = journey_df_filtered.sort_values(by='total_journey_distance_m').reset_index(drop=True)\n",
    "\n",
    "#     if top_options_df.empty: # Should not happen if journey_df_filtered was not empty, but good check\n",
    "#         return (f\"Could not determine top options for {origin_poi_name} to {destination_poi_name} after sorting.\")\n",
    "\n",
    "#     shortest_journey = top_options_df.iloc[0]\n",
    "\n",
    "#     # Construct the output string\n",
    "#     output_str = f\"Accessibility Information for {origin_poi_name} to {destination_poi_name}:\\n\"\n",
    "#     output_str += f\"- Calculated Accessibility Score: {accessibility_score:.2f}\\n\"\n",
    "#     output_str += f\"- This score is based on an exponential decay of the shortest unique journey distances.\\n\"\n",
    "    \n",
    "#     output_str += f\"\\nDetails of the overall shortest journey option found:\\n\"\n",
    "#     # Using .get with a default for robustness if a column is unexpectedly missing\n",
    "#     output_str += f\"  - Route ID(s) involved: {shortest_journey.get('route_id', 'N/A')}\\n\"\n",
    "#     output_str += f\"  - Origin Bus Stop ID: {shortest_journey.get('origin_stop_id', 'N/A')}\\n\"\n",
    "#     output_str += f\"  - Destination Bus Stop ID: {shortest_journey.get('destination_stop_id', 'N/A')}\\n\"\n",
    "    \n",
    "#     # Ensure float formatting only if the value is not 'N/A' (or handle potential non-numeric if 'N/A' is not from .get())\n",
    "#     walk_origin_dist = shortest_journey.get('walking_distance_from_origin_poi_m', 'N/A')\n",
    "#     transit_dist = shortest_journey.get('transit_distance_m', 'N/A')\n",
    "#     walk_dest_dist = shortest_journey.get('walking_distance_to_dest_poi_m', 'N/A')\n",
    "#     total_dist = shortest_journey.get('total_journey_distance_m', 'N/A')\n",
    "\n",
    "#     output_str += f\"  - Walking distance from {origin_poi_name}: {walk_origin_dist if isinstance(walk_origin_dist, str) else f'{walk_origin_dist:.2f}'} meters\\n\"\n",
    "#     output_str += f\"  - Transit distance: {transit_dist if isinstance(transit_dist, str) else f'{transit_dist:.2f}'} meters\\n\"\n",
    "#     output_str += f\"  - Walking distance to {destination_poi_name}: {walk_dest_dist if isinstance(walk_dest_dist, str) else f'{walk_dest_dist:.2f}'} meters\\n\"\n",
    "#     output_str += f\"  - Total Journey Distance: {total_dist if isinstance(total_dist, str) else f'{total_dist:.2f}'} meters\\n\"\n",
    "    \n",
    "#     unique_shortest_distances = top_options_df['total_journey_distance_m'].dropna().unique()\n",
    "#     if len(unique_shortest_distances) > 0:\n",
    "#         output_str += f\"\\nThe accessibility score considers the following shortest unique total journey distances (meters) from the data for this O-D pair:\\n\"\n",
    "#         for i, dist_val in enumerate(sorted(unique_shortest_distances)[:3]): # Show top 3 unique sorted\n",
    "#              output_str += f\"  - d{i+1}: {dist_val:.2f}\\n\"\n",
    "\n",
    "#     return output_str\n",
    "\n",
    "\n",
    "\n",
    "def get_journey_accessibility_info_v2(\n",
    "    origin_stop_id: str, \n",
    "    destination_stop_id: str,\n",
    "    connections_df: pd.DataFrame \n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Provides accessibility information for a journey between a specified origin_stop_id \n",
    "    and destination_stop_id using the provided connections_df.\n",
    "    It dynamically calculates the accessibility score and details the top journey option(s).\n",
    "    Retrieves POI names from the matched row for user-friendly output.\n",
    "    \"\"\"\n",
    "    if connections_df is None or connections_df.empty:\n",
    "        return \"Error: Input connections_df is not provided or is empty.\"\n",
    "\n",
    "    # Clean the input stop IDs (assuming they are strings)\n",
    "    clean_origin_stop_id = str(origin_stop_id).strip()\n",
    "    clean_destination_stop_id = str(destination_stop_id).strip()\n",
    "\n",
    "    print(f\"[get_journey_accessibility_info_v2 DEBUG] Filtering DataFrame for origin_stop_id='{clean_origin_stop_id}', destination_stop_id='{clean_destination_stop_id}'\")\n",
    "    \n",
    "    # Filter by stop_id. Assuming stop_id columns are strings or can be reliably cast to strings for comparison.\n",
    "    # If they are numbers, astype(str) might not be needed, or direct comparison could be used.\n",
    "    origin_id = '8460B5256901'\n",
    "    destination_ids_list = ['8460B5230201', '8460B5225601', '8460B522331']\n",
    "\n",
    "    journey_df_filtered = connections_df[\n",
    "        (connections_df['origin_stop_id'].astype(str) == origin_id) &\n",
    "        (connections_df['destination_stop_id'].astype(str).isin(destination_ids_list))\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"[get_journey_accessibility_info_v2 DEBUG] Rows found after filtering by stop_id: {len(journey_df_filtered)}\")\n",
    "\n",
    "    if journey_df_filtered.empty:\n",
    "        return (f\"No journey data found in the provided connections_df for \"\n",
    "                f\"origin_stop_id '{origin_stop_id}' to destination_stop_id '{destination_stop_id}'.\")\n",
    "\n",
    "    # Since we filtered by stop_id, we should retrieve the POI names from the found data\n",
    "    # Assuming there's one primary match or we take the first one.\n",
    "    # These will be used for the output message.\n",
    "    # Add error handling in case these columns don't exist or row is empty after all\n",
    "    try:\n",
    "        # Get POI names from the *first row* of the filtered data\n",
    "        # These are used for constructing the human-readable output message.\n",
    "        actual_origin_poi_name = journey_df_filtered.iloc[0]['origin_poi']\n",
    "        actual_destination_poi_name = journey_df_filtered.iloc[0]['destination_poi']\n",
    "    except (IndexError, KeyError) as e:\n",
    "        print(f\"[get_journey_accessibility_info_v2 DEBUG] Could not retrieve POI names from filtered data: {e}\")\n",
    "        actual_origin_poi_name = f\"Origin for stop {origin_stop_id}\" # Fallback\n",
    "        actual_destination_poi_name = f\"Destination for stop {destination_stop_id}\" # Fallback\n",
    "\n",
    "\n",
    "    try:\n",
    "        accessibility_score = calculate_exp_decay_accessibility_score(journey_df_filtered) \n",
    "    except Exception as e:\n",
    "        return f\"Error calculating accessibility score: {e}. Make sure 'calculate_exp_decay_accessibility_score' is correctly defined. Data for score calc had {len(journey_df_filtered)} rows.\"\n",
    "\n",
    "    top_options_df = journey_df_filtered.sort_values(by='total_journey_distance_m').reset_index(drop=True)\n",
    "\n",
    "    if top_options_df.empty: \n",
    "        return (f\"Could not determine top options for stop_id {origin_stop_id} to {destination_stop_id} after sorting.\")\n",
    "\n",
    "    shortest_journey = top_options_df.iloc[0]\n",
    "\n",
    "    # Use the retrieved POI names for the output\n",
    "    output_str = f\"Accessibility Information for journey from '{actual_origin_poi_name}' (Stop ID: {origin_stop_id}) to '{actual_destination_poi_name}' (Stop ID: {destination_stop_id}):\\n\"\n",
    "    output_str += f\"- Calculated Accessibility Score: {accessibility_score:.2f}\\n\"\n",
    "    output_str += f\"- This score is based on an exponential decay of the shortest unique journey distances.\\n\"\n",
    "    \n",
    "    output_str += f\"\\nDetails of the overall shortest journey option found:\\n\"\n",
    "    output_str += f\"  - Route ID(s) involved: {shortest_journey.get('route_id', 'N/A')}\\n\"\n",
    "    # The origin/destination stop_ids are now the primary query keys, so we definitely have them\n",
    "    output_str += f\"  - Origin Bus Stop ID: {clean_origin_stop_id}\\n\"\n",
    "    output_str += f\"  - Destination Bus Stop ID: {clean_destination_stop_id}\\n\"\n",
    "    \n",
    "    walk_origin_dist = shortest_journey.get('walking_distance_from_origin_poi_m', 'N/A')\n",
    "    transit_dist = shortest_journey.get('transit_distance_m', 'N/A')\n",
    "    walk_dest_dist = shortest_journey.get('walking_distance_to_dest_poi_m', 'N/A')\n",
    "    total_dist = shortest_journey.get('total_journey_distance_m', 'N/A')\n",
    "\n",
    "    def format_dist(val):\n",
    "        if isinstance(val, (int, float)) and not pd.isna(val):\n",
    "            return f\"{val:.2f}\"\n",
    "        return 'N/A'\n",
    "\n",
    "    output_str += f\"  - Walking distance from {actual_origin_poi_name}: {format_dist(walk_origin_dist)} meters\\n\"\n",
    "    output_str += f\"  - Transit distance: {format_dist(transit_dist)} meters\\n\"\n",
    "    output_str += f\"  - Walking distance to {actual_destination_poi_name}: {format_dist(walk_dest_dist)} meters\\n\"\n",
    "    output_str += f\"  - Total Journey Distance: {format_dist(total_dist)} meters\\n\"\n",
    "    \n",
    "    unique_shortest_distances = top_options_df['total_journey_distance_m'].dropna().unique()\n",
    "    if len(unique_shortest_distances) > 0:\n",
    "        output_str += f\"\\nThe accessibility score considers the following shortest unique total journey distances (meters) from the data for this O-D pair:\\n\"\n",
    "        for i, dist_val in enumerate(sorted(unique_shortest_distances)[:3]): \n",
    "             output_str += f\"  - d{i+1}: {dist_val:.2f}\\n\"\n",
    "\n",
    "    return output_str\n",
    "\n",
    "# def get_nearby_bus_stops_from_graph(G, poi_node_id: str, max_distance_meters: int = MAX_ACCESS_DISTANCE_METERS) -> str:\n",
    "#     \"\"\"\n",
    "#     Finds bus stops connected to a POI node in the graph 'G' via 'access_egress' \n",
    "#     edges within a specified maximum walking distance.\n",
    "#     The POI node ID must exist in the graph G.\n",
    "#     \"\"\"\n",
    "#     # Access the globally loaded graph G.\n",
    "#     # Make sure `G` is loaded in your script/notebook.\n",
    "#     if 'G' not in globals() or not isinstance(G, nx.DiGraph):\n",
    "#         return \"Error: Graph G is not loaded or is not a valid NetworkX DiGraph.\"\n",
    "\n",
    "#     if not G.has_node(poi_node_id):\n",
    "#         return f\"Error: POI node '{poi_node_id}' not found in the graph G.\"\n",
    "\n",
    "#     nearby_stops_info = []\n",
    "#     # Your notebook's logic for finding nearby stops (from Section 9)\n",
    "#     # Iterate over outgoing edges from the POI node\n",
    "#     if poi_node_id in G: # Check if node exists before querying edges\n",
    "#         for u, v, data in G.out_edges(poi_node_id, data=True):\n",
    "#             edge_type = data.get('type')\n",
    "#             edge_distance = data.get('distance_m', float('inf'))\n",
    "\n",
    "#             if edge_type == 'access_egress': # Check it's a walking/cycling edge\n",
    "#                 # Check if the connected node 'v' is a bus stop\n",
    "#                 if v in G and G.nodes[v].get('type') == 'bus_stop':\n",
    "#                     if edge_distance <= max_distance_meters:\n",
    "#                         stop_name = G.nodes[v].get('name', 'N/A') # Get bus stop name if available\n",
    "#                         nearby_stops_info.append({\n",
    "#                             'stop_id': v, \n",
    "#                             'stop_name': stop_name,\n",
    "#                             'distance_m': edge_distance\n",
    "#                         })\n",
    "    \n",
    "#     if not nearby_stops_info:\n",
    "#         return f\"No bus stops found within {max_distance_meters}m of '{poi_node_id}' in the graph.\"\n",
    "\n",
    "#     output_str = f\"Nearby bus stops for '{poi_node_id}' (within {max_distance_meters}m):\\n\"\n",
    "#     for stop_info in sorted(nearby_stops_info, key=lambda x: x['distance_m']): # Sort by distance\n",
    "#         output_str += (f\"  - Stop ID: {stop_info['stop_id']}, Name: {stop_info['stop_name']}, \"\n",
    "#                        f\"Distance: {stop_info['distance_m']:.2f} meters\\n\")\n",
    "    \n",
    "#     return output_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_journey_accessibility_info_v2 DEBUG] Filtering DataFrame for origin_stop_id='('8460B5256901',)', destination_stop_id='['8460B5230201', '8460B5225601', '8460B522331']'\n",
      "[get_journey_accessibility_info_v2 DEBUG] Rows found after filtering by stop_id: 3\n",
      "Using beta (decay parameter): 0.0001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Accessibility Information for journey from 'Rahoon' (Stop ID: ('8460B5256901',)) to 'Portershed a Dó' (Stop ID: ['8460B5230201', '8460B5225601', '8460B522331']):\\n- Calculated Accessibility Score: 35.88\\n- This score is based on an exponential decay of the shortest unique journey distances.\\n\\nDetails of the overall shortest journey option found:\\n  - Route ID(s) involved: 405\\n  - Origin Bus Stop ID: ('8460B5256901',)\\n  - Destination Bus Stop ID: ['8460B5230201', '8460B5225601', '8460B522331']\\n  - Walking distance from Rahoon: 726.46 meters\\n  - Transit distance: 5847.48 meters\\n  - Walking distance to Portershed a Dó: 656.22 meters\\n  - Total Journey Distance: 7230.16 meters\\n\\nThe accessibility score considers the following shortest unique total journey distances (meters) from the data for this O-D pair:\\n  - d1: 7230.16\\n  - d2: 14390.37\\n  - d3: 14905.19\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_stop_id= '8460B5256901', \n",
    "destination_stop_id= ['8460B5230201', '8460B5225601', '8460B522331']\n",
    "\n",
    "get_journey_accessibility_info_v2(origin_stop_id, destination_stop_id, place_to_building_connections_df)\n",
    "# get_journey_accessibility_info_v2(rahoon_node_id, portershed_node_id, place_to_building_connections_df)\n",
    "# display(get_nearby_bus_stops_from_graph(G, rahoon_node_id))\n",
    "# rahoon_node_id, portershed_node_id\n",
    "# place_to_building_connections_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 1 tools for the LangChain agent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/njindal/.pyenv/versions/3.10.13/envs/project3_10_13/lib/python3.10/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain ReAct Agent Executor created successfully and ready to use!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLet's get started!\n",
      "\n",
      "Thought: I need to use the `get_accessibility_and_journey_details` tool to retrieve the accessibility information for the given bus stop IDs.\n",
      "\n",
      "Action: get_accessibility_and_journey_details\n",
      "Action Input: origin_stop_id: 8460B5256901, destination_stop_id: 8460B5225601\n",
      "\u001b[0m\n",
      "[TOOL DEBUG] get_accessibility_and_journey_details received raw stop_id_query: 'origin_stop_id: 8460B5256901, destination_stop_id: 8460B5225601\n",
      "'\n",
      "[TOOL DEBUG] Parsed stop IDs: origin_stop_id='8460B5256901', destination_stop_id='8460B5225601'\n",
      "[TOOL DEBUG] Calling get_journey_accessibility_info_v2 with: origin_stop_id='8460B5256901', dest_stop_id='8460B5225601'\n",
      "[get_journey_accessibility_info_v2 DEBUG] Filtering DataFrame for origin_stop_id='8460B5256901', destination_stop_id='8460B5225601'\n",
      "[get_journey_accessibility_info_v2 DEBUG] Rows found after filtering by stop_id: 3\n",
      "Using beta (decay parameter): 0.0001\n",
      "[TOOL DEBUG] Result from get_journey_accessibility_info_v2 (first 100 chars): Accessibility Information for journey from 'Rahoon' (Stop ID: 8460B5256901) to 'Portershed a Dó' (St...\n",
      "\u001b[36;1m\u001b[1;3mAccessibility Information for journey from 'Rahoon' (Stop ID: 8460B5256901) to 'Portershed a Dó' (Stop ID: 8460B5225601):\n",
      "- Calculated Accessibility Score: 35.88\n",
      "- This score is based on an exponential decay of the shortest unique journey distances.\n",
      "\n",
      "Details of the overall shortest journey option found:\n",
      "  - Route ID(s) involved: 405\n",
      "  - Origin Bus Stop ID: 8460B5256901\n",
      "  - Destination Bus Stop ID: 8460B5225601\n",
      "  - Walking distance from Rahoon: 726.46 meters\n",
      "  - Transit distance: 5847.48 meters\n",
      "  - Walking distance to Portershed a Dó: 656.22 meters\n",
      "  - Total Journey Distance: 7230.16 meters\n",
      "\n",
      "The accessibility score considers the following shortest unique total journey distances (meters) from the data for this O-D pair:\n",
      "  - d1: 7230.16\n",
      "  - d2: 14390.37\n",
      "  - d3: 14905.19\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mI now know that I need to provide an accessibility score and journey details based on the input question.\n",
      "\n",
      "Thought: The `get_accessibility_and_journey_details` tool has provided me with the required information, which I will use as my Final Answer.\n",
      "\n",
      "Final Answer: The accessibility score for the journey from origin_stop_id: 8460B5256901 to destination_stop_id: 8460B5225601 is 35.88. Additionally, the journey details indicate that the route ID(s) involved are 405, with walking distances of 726.46 meters and 656.22 meters respectively, a total transit distance of 5847.48 meters, and a total journey distance of 7230.16 meters.\n",
      "\n",
      "As for recommendations based on my knowledge, I would suggest considering alternative routes or modes of transport if the accessibility score is below a certain threshold (e.g., 40), as this may indicate that the chosen route has significant barriers to accessibility. However, in this case, the score suggests that the chosen route is relatively accessible.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The accessibility score for the journey from origin_stop_id: 8460B5256901 to destination_stop_id: 8460B5225601 is 35.88. Additionally, the journey details indicate that the route ID(s) involved are 405, with walking distances of 726.46 meters and 656.22 meters respectively, a total transit distance of 5847.48 meters, and a total journey distance of 7230.16 meters.\n",
      "\n",
      "As for recommendations based on my knowledge, I would suggest considering alternative routes or modes of transport if the accessibility score is below a certain threshold (e.g., 40), as this may indicate that the chosen route has significant barriers to accessibility. However, in this case, the score suggests that the chosen route is relatively accessible.\n"
     ]
    }
   ],
   "source": [
    "# class JourneyDetailsInput(BaseModel):\n",
    "#     origin_poi_name: str = Field(description=\"The name of the Place, for example, 'Rahoon'\")\n",
    "#     destination_poi_name: str = Field(description=\"The name of the building, for example, 'Portershed a Dó'\")\n",
    "\n",
    "\n",
    "# @tool(args_schema=JourneyDetailsInput)\n",
    "# def get_accessibility_and_journey_details(\n",
    "#     origin_poi_name: str, \n",
    "#     destination_poi_name: str,\n",
    "# ) -> str:\n",
    "#     \"\"\"\n",
    "#     Provides detailed accessibility information for a public transport journey \n",
    "#     between a specified origin Point of Interest (POI) and a destination POI.\n",
    "#     It returns the dynamically calculated accessibility score and details of \n",
    "#     the top contributing journey options based on the pre-loaded \n",
    "#     'place_to_building_connections_df' DataFrame. \n",
    "#     The origin_poi_name and destination_poi_name must match entries in the DataFrame \n",
    "#     (e.g., 'Rahoon', 'Portershed a Dó').\n",
    "#     \"\"\"\n",
    "#     # Ensure place_to_building_connections_df is accessible\n",
    "#     if 'place_to_building_connections_df' not in globals() or place_to_building_connections_df.empty:\n",
    "#         return \"Error: Main journey data (place_to_building_connections_df) is not loaded or is empty.\"\n",
    "    \n",
    "#     # Call your actual function - it now takes connections_df from the global scope\n",
    "#     return get_journey_accessibility_info_v2(origin_poi_name, destination_poi_name, place_to_building_connections_df) \n",
    "  \n",
    "# @tool\n",
    "# def get_accessibility_and_journey_details(origin_and_destination_query: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Provides detailed accessibility information for a public transport journey \n",
    "#     between an origin and a destination. \n",
    "#     The input 'origin_and_destination_query' should be a string clearly stating \n",
    "#     the origin and destination, for example: 'journey from Rahoon to Portershed a Dó' \n",
    "#     or simply 'Rahoon to Portershed a Dó'. \n",
    "#     The tool will attempt to parse out the origin and destination names.\n",
    "#     \"\"\"\n",
    "#     cleaned_query = origin_and_destination_query.strip()\n",
    "#     origin_poi_name = None\n",
    "#     destination_poi_name = None\n",
    "\n",
    "#     match_from_to = re.search(r\"(?:from\\s+)?(.*?)\\s+to\\s+(.*)\", cleaned_query, re.IGNORECASE)\n",
    "#     if match_from_to:\n",
    "#         origin_poi_name = match_from_to.group(1).strip()\n",
    "#         destination_poi_name = match_from_to.group(2).strip()\n",
    "#     else:\n",
    "#         return (f\"Error: Could not reliably parse origin and destination from: '{cleaned_query}'. \"\n",
    "#                 f\"Please phrase input like 'Origin to Destination'.\")\n",
    "\n",
    "#     if not origin_poi_name or not destination_poi_name:\n",
    "#         return (f\"Error: Failed to extract both origin ('{origin_poi_name}') and destination ('{destination_poi_name}') \"\n",
    "#                 f\"from input: '{cleaned_query}'.\")\n",
    "\n",
    "#     if 'place_to_building_connections_df' not in globals() or place_to_building_connections_df.empty:\n",
    "#         return \"Error: Main journey data (place_to_building_connections_df) is not loaded or is empty.\"\n",
    "    \n",
    "#     return get_journey_accessibility_info_v2(\n",
    "#         origin_poi_name,\n",
    "#         destination_poi_name,\n",
    "#         place_to_building_connections_df\n",
    "#     )\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def get_accessibility_and_journey_details(origin_and_destination_query: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Provides detailed accessibility information for a public transport journey \n",
    "#     between an origin and a destination. \n",
    "#     The input 'origin_and_destination_query' should be a string clearly stating \n",
    "#     the origin and destination, for example: 'journey from Rahoon to Portershed a Dó' \n",
    "#     or simply 'Rahoon to Portershed a Dó'. \n",
    "#     The tool will attempt to parse out the origin and destination names.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n[TOOL DEBUG] get_accessibility_and_journey_details received raw input: '{origin_and_destination_query}'\") \n",
    "#     cleaned_query = origin_and_destination_query.strip()\n",
    "#     print(f\"[TOOL DEBUG] Cleaned query: '{cleaned_query}'\")\n",
    "    \n",
    "#     origin_poi_name = None\n",
    "#     destination_poi_name = None\n",
    "\n",
    "#     # Regex to capture origin and destination\n",
    "#     # Tries to match \"from [ORIGIN] to [DESTINATION]\" or \"[ORIGIN] to [DESTINATION]\"\n",
    "#     match_from_to = re.search(r\"(?:from\\s+)?(.*?)\\s+to\\s+(.*)\", cleaned_query, re.IGNORECASE)\n",
    "    \n",
    "#     if match_from_to:\n",
    "#         origin_poi_name = match_from_to.group(1).strip()\n",
    "#         destination_poi_name = match_from_to.group(2).strip()\n",
    "#         print(f\"[TOOL DEBUG] Parsed by regex: origin='{origin_poi_name}', destination='{destination_poi_name}'\")\n",
    "#     else:\n",
    "#         # Fallback parsing attempt if \" to \" isn't present.\n",
    "#         # This is a simple fallback and might need to be more robust based on LLM output patterns.\n",
    "#         parts = cleaned_query.split(',') # Example: \"Rahoon, Portershed a Do\"\n",
    "#         if len(parts) == 2:\n",
    "#             origin_poi_name = parts[0].strip()\n",
    "#             destination_poi_name = parts[1].strip()\n",
    "#             print(f\"[TOOL DEBUG] Parsed by comma split: origin='{origin_poi_name}', destination='{destination_poi_name}'\")\n",
    "#         else:\n",
    "#             print(f\"[TOOL DEBUG] Parsing failed for: '{cleaned_query}' using regex and comma split.\")\n",
    "#             return (f\"Error: Could not reliably parse origin and destination from the input: '{cleaned_query}'. \"\n",
    "#                     f\"Please ensure the input clearly separates the origin and destination, ideally using 'from [Origin] to [Destination]' or '[Origin] to [Destination]'.\")\n",
    "\n",
    "#     if not origin_poi_name or not destination_poi_name:\n",
    "#         return (f\"Error: Failed to extract both valid origin ('{origin_poi_name}') and destination ('{destination_poi_name}') \"\n",
    "#                 f\"from input: '{cleaned_query}'.\")\n",
    "\n",
    "#     # Check for the DataFrame in globals\n",
    "#     if 'place_to_building_connections_df' not in globals() or globals()['place_to_building_connections_df'].empty:\n",
    "#         print(\"[TOOL DEBUG] Error: place_to_building_connections_df not found or empty in global scope.\")\n",
    "#         return \"Error: Main journey data (place_to_building_connections_df) is not loaded or is empty.\"\n",
    "    \n",
    "#     # Access the global DataFrame\n",
    "#     current_connections_df = globals()['place_to_building_connections_df']\n",
    "    \n",
    "#     print(f\"[TOOL DEBUG] Calling get_journey_accessibility_info_v2 with: origin='{origin_poi_name}', dest='{destination_poi_name}'\")\n",
    "#     result = get_journey_accessibility_info_v2(\n",
    "#         origin_poi_name, \n",
    "#         destination_poi_name, \n",
    "#         current_connections_df # Pass the actual DataFrame\n",
    "#     )\n",
    "#     print(f\"[TOOL DEBUG] Result from get_journey_accessibility_info_v2 (first 100 chars): {result[:100]}...\")\n",
    "#     return result\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_accessibility_and_journey_details(stop_id_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Provides detailed accessibility information for a public transport journey \n",
    "    given an origin bus stop ID and a destination bus stop ID.\n",
    "    The input 'stop_id_query' should be a string clearly stating these IDs, \n",
    "    for example: 'origin_stop_id: XXXXX, destination_stop_id: YYYYY' \n",
    "    or 'journey from stop XXXXX to stop YYYYY'.\n",
    "    The tool will attempt to parse out the origin and destination stop IDs.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[TOOL DEBUG] get_accessibility_and_journey_details received raw stop_id_query: '{stop_id_query}'\")\n",
    "    cleaned_query = stop_id_query.strip()\n",
    "    \n",
    "    origin_stop_id = None\n",
    "    destination_stop_id = None\n",
    "\n",
    "    # Try to parse \"origin_stop_id: XXXXX, destination_stop_id: YYYYY\" format\n",
    "    match_kv = re.search(r\"origin_stop_id:\\s*([\\w\\d]+)\\s*,\\s*destination_stop_id:\\s*([\\w\\d]+)\", cleaned_query, re.IGNORECASE)\n",
    "    if match_kv:\n",
    "        origin_stop_id = match_kv.group(1).strip()\n",
    "        destination_stop_id = match_kv.group(2).strip()\n",
    "    else:\n",
    "        # Try to parse \"from stop XXXXX to stop YYYYY\" or \"stop XXXXX to stop YYYYY\"\n",
    "        match_from_to = re.search(r\"(?:from\\s+stop\\s+|stop\\s+)([\\w\\d]+)\\s+to\\s+(?:stop\\s+)?([\\w\\d]+)\", cleaned_query, re.IGNORECASE)\n",
    "        if match_from_to:\n",
    "            origin_stop_id = match_from_to.group(1).strip()\n",
    "            destination_stop_id = match_from_to.group(2).strip()\n",
    "        else: # A more general attempt if specific keywords are missing\n",
    "            parts = re.findall(r'([\\w\\d]+)', cleaned_query) # Find all alphanumeric sequences\n",
    "            if len(parts) >= 2: # Take the first two found as potential IDs if other parsing fails\n",
    "                 # This is a very liberal parsing, assumes IDs are among the first alphanumeric words\n",
    "                potential_ids = [p for p in parts if any(char.isdigit() for char in p) and len(p) > 4] # Basic check for ID-like strings\n",
    "                if len(potential_ids) >=2:\n",
    "                    origin_stop_id = potential_ids[0]\n",
    "                    destination_stop_id = potential_ids[1]\n",
    "                else:\n",
    "                    print(f\"[TOOL DEBUG] Could not find two distinct ID-like parts in '{cleaned_query}'\")\n",
    "\n",
    "\n",
    "    print(f\"[TOOL DEBUG] Parsed stop IDs: origin_stop_id='{origin_stop_id}', destination_stop_id='{destination_stop_id}'\")\n",
    "\n",
    "    if not origin_stop_id or not destination_stop_id:\n",
    "        return (f\"Error: Could not reliably parse both origin_stop_id and destination_stop_id from: '{cleaned_query}'. \"\n",
    "                f\"Please use formats like 'origin_stop_id: X, destination_stop_id: Y' or 'from stop X to stop Y'.\")\n",
    "\n",
    "    if 'place_to_building_connections_df' not in globals() or globals()['place_to_building_connections_df'].empty:\n",
    "        print(\"[TOOL DEBUG] Error: place_to_building_connections_df not found or empty.\")\n",
    "        return \"Error: Main journey data (place_to_building_connections_df) is not loaded or is empty.\"\n",
    "    \n",
    "    current_connections_df = globals()['place_to_building_connections_df']\n",
    "    \n",
    "    print(f\"[TOOL DEBUG] Calling get_journey_accessibility_info_v2 with: origin_stop_id='{origin_stop_id}', dest_stop_id='{destination_stop_id}'\")\n",
    "    result = get_journey_accessibility_info_v2(\n",
    "        origin_stop_id,\n",
    "        destination_stop_id,\n",
    "        current_connections_df\n",
    "    )\n",
    "    print(f\"[TOOL DEBUG] Result from get_journey_accessibility_info_v2 (first 100 chars): {result[:100]}...\")\n",
    "    return result\n",
    "\n",
    "\n",
    " \n",
    "# @tool\n",
    "# def find_nearby_bus_stops(poi_node_id: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Finds bus stops connected to a Point of Interest (POI) node in the transport graph 'G' \n",
    "#     via 'access_egress' (walking) edges, within a default maximum distance of \n",
    "#     800 meters. The poi_node_id must be an exact match to a node ID in the graph G\n",
    "#     (e.g., 'Rahoon' for a place, or a specific building name like 'Portershed a Dó').\n",
    "#     Returns a list of nearby stop IDs, their names, and distances.\n",
    "#     \"\"\"\n",
    "#     # Ensure G is accessible\n",
    "#     if 'G' not in globals() or not isinstance(G, nx.DiGraph):\n",
    "#         return \"Error: Transport graph G is not loaded or is not a valid NetworkX DiGraph.\"\n",
    "        \n",
    "#     # Call your actual function\n",
    "#     return get_nearby_bus_stops_from_graph(G, poi_node_id, MAX_ACCESS_DISTANCE_METERS)\n",
    "\n",
    "# create a list of these tools for the agent\n",
    "# tools = [find_nearby_bus_stops]\n",
    "tools = [get_accessibility_and_journey_details]\n",
    "# tools = [get_accessibility_and_journey_details, find_nearby_bus_stops]\n",
    "print(f\"Defined {len(tools)} tools for the LangChain agent.\")\n",
    "\n",
    "\n",
    "# Pull a standard ReAct prompt template\n",
    "# This prompt provides the LLM with instructions on how to reason and use tools.\n",
    "prompt_template = hub.pull(\"hwchase17/react\") # Harrison Chase Reasoning and Acting agent framework(ReAct)\n",
    "prompt_template\n",
    "agent = create_react_agent(llm, tools, prompt_template)\n",
    "\n",
    "#LangChain Agent\n",
    "# The AgentExecutor runs the agent, calls tools, and gets responses\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, # Set to True to see the agent's thought process and actions\n",
    "    handle_parsing_errors=True, # Helps with robustness if LLM output is not perfectly formatted\n",
    "    max_iterations=5 # Prevents runaway agents if it gets stuck in a loop, adjust as needed\n",
    ")\n",
    "print(\"LangChain ReAct Agent Executor created successfully and ready to use!\")\n",
    "\n",
    "\n",
    "# --Query 1 --\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": \"What is the accessibility score and journey details for origin_stop_id: 8460B5256901, destination_stop_id: 8460B5225601? Would you like to give any recommendations based on your knowledge?\"\n",
    "})\n",
    "print(response[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull a standard ReAct prompt template\n",
    "# This prompt provides the LLM with instructions on how to reason and use tools.\n",
    "prompt_template = hub.pull(\"hwchase17/react\") # Harrison Chase Reasoning and Acting agent framework(ReAct)\n",
    "prompt_template\n",
    "agent = create_react_agent(llm, tools, prompt_template)\n",
    "\n",
    "# The AgentExecutor runs the agent, calls tools, and gets responses\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, # Set to True to see the agent's thought process and actions\n",
    "    handle_parsing_errors=True, # Helps with robustness if LLM output is not perfectly formatted\n",
    "    max_iterations=5 # Prevents runaway agents if it gets stuck in a loop, adjust as needed\n",
    ")\n",
    "print(\"LangChain ReAct Agent Executor created successfully and ready to use!\")\n",
    "\n",
    "\n",
    "# --Query 1 --\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": \"What is the accessibility score and journey details for origin_stop_id: 8460B5256901, destination_stop_id: 8460B5225601? Give two recommendations based on your knowledge?\"\n",
    "})\n",
    "print(response[\"output\"])\n",
    "\n",
    "\n",
    "# print(\"\\n--- Query 1: Accessibility Details ---\")\n",
    "# response1 = agent_executor.invoke({\n",
    "#     \"input\": \"What is the accessibility score and the shortest journey details for a trip from Rahoon to Portershed a Dó?\"\n",
    "# })\n",
    "# print(\"\\nAgent's Final Answer:\")\n",
    "# print(response1[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # --Query 2 --\n",
    "# print(\"\\n--- Query 2: Nearby Stops for Rahoon ---\")\n",
    "# response2 = agent_executor.invoke({\n",
    "#     \"input\": \"Can you find the bus stops near Rahoon?\"\n",
    "# })\n",
    "# print(\"\\nAgent's Final Answer:\")\n",
    "# print(response2[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Rahoon-Portershed Public Transport Accessibility Map for Galway - (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Find the Shortest Journey ---\n",
    "# shortest_journey_path_details = None # To store details of the shortest path\n",
    "\n",
    "# if 'place_to_building_connections_df' in locals() and \\\n",
    "#    not place_to_building_connections_df.empty and \\\n",
    "#    'total_journey_distance_m' in place_to_building_connections_df.columns:\n",
    "\n",
    "#     # Drop rows where total journey distance is NaN\n",
    "#     valid_journeys_df = place_to_building_connections_df.dropna(subset=['total_journey_distance_m']).copy()\n",
    "    \n",
    "#     if not valid_journeys_df.empty:\n",
    "#         valid_journeys_df = valid_journeys_df.sort_values(by='total_journey_distance_m')\n",
    "#         shortest_journey_row = valid_journeys_df.iloc[0].copy() # Get the top row (shortest)\n",
    "\n",
    "#         # --- Store the necessary details for plotting ---\n",
    "#         shortest_journey_path_details = {\n",
    "#             'origin_poi_node_id': rahoon_node_id, # variable for Rahoon POI node ID\n",
    "#             'origin_bus_stop_id': shortest_journey_row['origin_stop_id'],\n",
    "#             'route_id': shortest_journey_row['route_id'],\n",
    "#             'origin_bus_stop_order': shortest_journey_row['origin_stop_order'],\n",
    "#             'destination_bus_stop_order': shortest_journey_row['destination_stop_order'],\n",
    "#             'destination_bus_stop_id': shortest_journey_row['destination_stop_id'],\n",
    "#             'destination_poi_node_id': portershed_node_id, # var for Portershed POI\n",
    "#             'total_distance': shortest_journey_row['total_journey_distance_m']\n",
    "#         }\n",
    "#         print(\"\\n--- Shortest Journey Details for Plotting ---\")\n",
    "#         print(f\"Origin POI: {shortest_journey_path_details['origin_poi_node_id']}\")\n",
    "#         print(f\"Origin Bus Stop: {shortest_journey_path_details['origin_bus_stop_id']}\")\n",
    "#         print(f\"Route ID: {shortest_journey_path_details['route_id']}\")\n",
    "#         print(f\"Destination Bus Stop: {shortest_journey_path_details['destination_bus_stop_id']}\")\n",
    "#         print(f\"Destination POI: {shortest_journey_path_details['destination_poi_node_id']}\")\n",
    "#         print(f\"Total Distance: {shortest_journey_path_details['total_distance']:.2f}m\")\n",
    "\n",
    "#         # Get the sequence of transit stops for this shortest path\n",
    "#         route_seq_df_shortest = bus_timetables[\n",
    "#             bus_timetables['route_id'] == shortest_journey_path_details['route_id']\n",
    "#         ].sort_values(by='stop_order_on_route')\n",
    "        \n",
    "#         path_segment_df_shortest = route_seq_df_shortest[\n",
    "#             (route_seq_df_shortest['stop_order_on_route'] >= shortest_journey_path_details['origin_bus_stop_order']) &\n",
    "#             (route_seq_df_shortest['stop_order_on_route'] <= shortest_journey_path_details['destination_bus_stop_order'])\n",
    "#         ]\n",
    "#         shortest_journey_path_details['transit_stop_sequence_ids'] = path_segment_df_shortest['stop_id_mapped'].tolist()\n",
    "#         print(f\"Transit Stop Sequence: {shortest_journey_path_details['transit_stop_sequence_ids']}\")\n",
    "#     else:\n",
    "#         print(\"No valid journeys with calculated total distances found to select the shortest.\")\n",
    "# else:\n",
    "#     print(\"place_to_building_connections_df not found, empty, or 'total_journey_distance_m' column missing.\")\n",
    "\n",
    "\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "# # Configure osmnx settings and logging\n",
    "# ox.config(log_console=True, use_cache=False)\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # --- Configuration ---\n",
    "# place_name = \"Galway, Ireland\"\n",
    "# shapefile_base_dir = '/Users/njindal/Documents/aic2025/data/ireland-and-northern-ireland-latest-free.shp' \n",
    "\n",
    "# shapefile_layers = {\n",
    "#     'roads': 'gis_osm_roads_free_1.shp',\n",
    "#     'water_poly': 'gis_osm_water_a_free_1.shp',\n",
    "#     'railways': 'gis_osm_railways_free_1.shp',\n",
    "#     'waterways': 'gis_osm_waterways_free_1.shp',\n",
    "#     'landuse': 'gis_osm_landuse_a_free_1.shp',\n",
    "#     'buildings': 'gis_osm_buildings_a_free_1.shp',\n",
    "#     'places_poly': 'gis_osm_places_a_free_1.shp'\n",
    "# }\n",
    "\n",
    "# print(f\"\\n--- Processing Data for: {place_name} ---\")\n",
    "# print(f\"Using Shapefile directory: {shapefile_base_dir}\")\n",
    "\n",
    "# try:\n",
    "#     # --- *** GET GALWAY BOUNDARY *** ---\n",
    "#     print(\"\\nFetching boundary for Galway...\")\n",
    "#     boundary_gdf = ox.geocode_to_gdf(place_name).to_crs(\"EPSG:4326\")\n",
    "#     if boundary_gdf.empty:\n",
    "#         raise ValueError(f\"Could not geocode '{place_name}'.\")\n",
    "#     print(f\"Boundary fetched. CRS set to: {boundary_gdf.crs}\")\n",
    "\n",
    "\n",
    "\n",
    "#     # --- *** LOAD IRELAND SHAPEFILES & CLIP TO GALWAY BOUNDARY *** ---\n",
    "#     print(\"\\nLoading and clipping Ireland-wide layers to Galway boundary...\")\n",
    "#     galway_gdfs = {}\n",
    "#     for layer_name, shp_filename in shapefile_layers.items():\n",
    "#         shp_path = os.path.join(shapefile_base_dir, shp_filename)\n",
    "#         print(f\"--- Processing layer: {layer_name} ---\")\n",
    "#         if not os.path.exists(shp_path):\n",
    "#             print(f\"*** WARNING: Shapefile not found: {shp_path} - Skipping layer '{layer_name}' ***\")\n",
    "#             continue\n",
    "#         try:\n",
    "#             ireland_layer_gdf = gpd.read_file(shp_path)\n",
    "#             if ireland_layer_gdf.crs != boundary_gdf.crs:\n",
    "#                 ireland_layer_gdf = ireland_layer_gdf.to_crs(boundary_gdf.crs)\n",
    "#             clipped_gdf = gpd.clip(ireland_layer_gdf, boundary_gdf, keep_geom_type=True)\n",
    "#             if not clipped_gdf.empty:\n",
    "#                 galway_gdfs[layer_name] = clipped_gdf\n",
    "#             else:\n",
    "#                 print(f\"Note: No features found for layer '{layer_name}'.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"*** ERROR processing layer '{layer_name}': {e} ***\")\n",
    "\n",
    "\n",
    "\n",
    "#     # --- *** PREPARE BUS STOP GEODATAFRAME FROM GSTOPS_DF_V1 *** --- \n",
    "#     print(\"\\nPreparing Galway Bus Stop data from gstops_df_v1...\")\n",
    "#     bus_stops_gdf = None\n",
    "#     if 'gstops_df_v1' in locals() and isinstance(gstops_df_v1, pd.DataFrame) and not gstops_df_v1.empty:\n",
    "#         # Ensure 'stop_lat' and 'stop_lon' columns exist\n",
    "#         if 'stop_lat' in gstops_df_v1.columns and 'stop_lon' in gstops_df_v1.columns:\n",
    "#             try:\n",
    "#                 # Drop rows with invalid (NaN) coordinates before creating GeoDataFrame\n",
    "#                 temp_stops_df = gstops_df_v1.dropna(subset=['stop_lat', 'stop_lon']).copy()\n",
    "                \n",
    "#                 if not temp_stops_df.empty:\n",
    "#                     bus_stops_gdf = gpd.GeoDataFrame(\n",
    "#                         temp_stops_df,\n",
    "#                         geometry=gpd.points_from_xy(temp_stops_df['stop_lon'], temp_stops_df['stop_lat']),\n",
    "#                         crs=\"EPSG:4326\"  \n",
    "#                     )\n",
    "#                     print(f\"Created GeoDataFrame 'bus_stops_gdf' with {len(bus_stops_gdf)} stops from gstops_df_v1.\")\n",
    "#                     # Reproject if CRS doesn't match the boundary CRS\n",
    "#                     if bus_stops_gdf.crs != boundary_gdf.crs:\n",
    "#                         print(f\"Reprojecting bus stops GDF to {boundary_gdf.crs}...\");\n",
    "#                         bus_stops_gdf = bus_stops_gdf.to_crs(boundary_gdf.crs)\n",
    "#                         print(\"Reprojection complete.\")\n",
    "#                 else:\n",
    "#                     print(\"Warning: No valid coordinates found in gstops_df_v1 after cleaning.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"*** ERROR converting gstops_df_v1 data: {e} ***\")\n",
    "#                 bus_stops_gdf = None\n",
    "#         else:\n",
    "#             print(\"Warning: 'stop_lat' or 'stop_lon' columns not found in gstops_df_v1.\")\n",
    "#     else:\n",
    "#         print(\"Warning: 'gstops_df_v1' DataFrame not found or is empty. Please load it first.\")\n",
    "\n",
    "\n",
    "#     # --- *** PREPARE BUS ROUTES GEODATAFRAME FROM gvariations_df_v1 *** --- \n",
    "#     bus_routes_gdf = None # Initialize\n",
    "\n",
    "#     if 'gvariations_df_v1' in locals() and isinstance(gvariations_df_v1, pd.DataFrame) and not gvariations_df_v1.empty and \\\n",
    "#     'bus_stops_gdf' in locals() and isinstance(bus_stops_gdf, gpd.GeoDataFrame) and not bus_stops_gdf.empty:\n",
    "\n",
    "#         print(\"\\nEnriching gvariations_df_v1 with first/last stop Point geometries...\")\n",
    "        \n",
    "#         bus_routes_gdf = gvariations_df_v1.copy()\n",
    "        \n",
    "#         if 'stop_id' in bus_stops_gdf.columns and 'geometry' in bus_stops_gdf.columns:\n",
    "            \n",
    "#             # --- Handle duplicate stop_ids in bus_stops_gdf to get a unique map ---\n",
    "#             # A single physical stop_id has one location, regardless of how many route directions use it.\n",
    "#             # We keep the first occurrence of each stop_id to get its unique geometry.\n",
    "#             bus_stops_gdf_unique_locations = bus_stops_gdf.drop_duplicates(subset=['stop_id'], keep='first')\n",
    "            \n",
    "#             # Create the mapping series from this de-duplicated DataFrame\n",
    "#             stop_id_to_point_geometry = bus_stops_gdf_unique_locations.set_index('stop_id')['geometry']\n",
    "            \n",
    "#         else:\n",
    "#             print(\"Error: 'stop_id' or 'geometry' column not found in bus_stops_gdf. Cannot map stop Point geometries.\")\n",
    "#             stop_id_to_point_geometry = pd.Series(dtype='object') \n",
    "\n",
    "#         # Map first stop Point geometry\n",
    "#         bus_routes_gdf['first_stop_point'] = bus_routes_gdf['first_stop_id'].map(stop_id_to_point_geometry)\n",
    "        \n",
    "#         # Map last stop Point geometry\n",
    "#         bus_routes_gdf['last_stop_point'] = bus_routes_gdf['last_stop_id'].map(stop_id_to_point_geometry)\n",
    "        \n",
    "#         num_first_stops_mapped = bus_routes_gdf['first_stop_point'].notna().sum()\n",
    "#         num_last_stops_mapped = bus_routes_gdf['last_stop_point'].notna().sum()\n",
    "        \n",
    "#         print(f\"Successfully mapped Point geometry for {num_first_stops_mapped} first stops.\")\n",
    "#         print(f\"Successfully mapped Point geometry for {num_last_stops_mapped} last stops.\")\n",
    "\n",
    "#         # Check if any mappings failed (resulting in NaNs)\n",
    "#         if bus_routes_gdf['first_stop_point'].isnull().any() or bus_routes_gdf['last_stop_point'].isnull().any():\n",
    "#             print(\"Warning: Some first/last stop points could not be mapped (resulting in NaNs).\")\n",
    "#             # You could print these rows for inspection:\n",
    "#             # print(bus_routes_gdf[bus_routes_gdf['first_stop_point'].isnull() | bus_routes_gdf['last_stop_point'].isnull()])\n",
    "\n",
    "\n",
    "#         print(\"\\n--- bus_routes_gdf (with Point geometries) ---\")\n",
    "#         # Display relevant columns to check the mapping\n",
    "#         display_cols = ['first_stop_id', 'first_stop_point', 'last_stop_id', 'last_stop_point']\n",
    "#         # Add other columns from gvariations_df_v1 if they provide context\n",
    "#         if 'route_id' in bus_routes_gdf.columns: display_cols.insert(0, 'route_id')\n",
    "#         if 'direction_id' in bus_routes_gdf.columns: display_cols.insert(1, 'direction_id')\n",
    "\n",
    "#         print(bus_routes_gdf[display_cols].head())\n",
    "#         print(f\"Shape of bus_routes_gdf: {bus_routes_gdf.shape}\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"\\nPrerequisite DataFrames ('gvariations_df_v1' or 'bus_stops_gdf') not available or empty. Cannot create bus_routes_gdf.\")\n",
    "\n",
    "\n",
    "#       # --- *** CREATE PLACE SUMMARY DATAFRAME *** ---\n",
    "#     print(\"\\nCreating DataFrame for Galway Place Names and Coordinates...\")\n",
    "#     galway_places_summary_df = None # Initialize\n",
    "#     if 'places_poly' in galway_gdfs and not galway_gdfs['places_poly'].empty:\n",
    "#         places_data = []\n",
    "#         # Check if the 'name' column exists\n",
    "#         if 'name' not in galway_gdfs['places_poly'].columns:\n",
    "#             print(\"Warning: 'name' column not found in places_poly layer. Cannot extract place names.\")\n",
    "#         else:\n",
    "#             # Iterate through valid polygons with names\n",
    "#             for idx, row in galway_gdfs['places_poly'][galway_gdfs['places_poly']['name'].notna() & galway_gdfs['places_poly'].geometry.is_valid].iterrows():\n",
    "#                 place_name_val = row['name']; geometry = row.geometry; rep_point = None\n",
    "#                 # Get representative point (or centroid as fallback)\n",
    "#                 if hasattr(geometry, 'representative_point'):\n",
    "#                     try: rep_point = geometry.representative_point()\n",
    "#                     except Exception: rep_point = geometry.centroid # Fallback if representative_point fails\n",
    "#                 else: rep_point = geometry.centroid # Fallback if method doesn't exist\n",
    "#                 # Append if point is valid\n",
    "#                 if rep_point and rep_point.is_valid:\n",
    "#                     places_data.append({'place_name': place_name_val,'latitude': rep_point.y,'longitude': rep_point.x})\n",
    "#             # Create DataFrame if data was extracted\n",
    "#             if places_data:\n",
    "#                 galway_places_summary_df = pd.DataFrame(places_data)\n",
    "#                 print(f\"Created DataFrame 'galway_places_summary_df' with {len(galway_places_summary_df)} places.\")\n",
    "#                 print(galway_places_summary_df.head())\n",
    "#             else: print(\"No valid places with names found to create summary DataFrame.\")\n",
    "#     else: print(\"Clipped 'places_poly' GeoDataFrame not found or is empty.\")\n",
    "\n",
    "#     galway_places_summary_df1 = None # Initialize\n",
    "\n",
    "#     if 'galway_places_summary_df' in locals() and isinstance(galway_places_summary_df, pd.DataFrame) and not galway_places_summary_df.empty:\n",
    "#         galway_places_summary_df1 = galway_places_summary_df.copy()\n",
    "#         if 'place_name' in galway_places_summary_df1.columns:\n",
    "#             galway_places_summary_df1 = galway_places_summary_df1.sort_values('place_name').reset_index(drop=True)\n",
    "#         else:\n",
    "#             print(\"Warning: 'place_name' column not found for sorting. Index will be based on current order.\")\n",
    "\n",
    "#         # Create custom indices starting with 'P'\n",
    "#         place_indices = [f'P{i+1}' for i in range(len(galway_places_summary_df1))]\n",
    "#         galway_places_summary_df1.index = place_indices\n",
    "\n",
    "#         print(\"\\nCreated DataFrame 'galway_places_summary_df1' with custom 'P' indices:\")\n",
    "#         print(f\"Number of places: {len(galway_places_summary_df1)}\")\n",
    "#         print(\"\\nFirst few rows of 'galway_places_summary_df1':\")\n",
    "#         print(galway_places_summary_df1.head())\n",
    "#     else:\n",
    "#         print(\"Cannot create 'galway_places_summary_df1' as 'galway_places_summary_df' is not available or is empty.\")\n",
    "#     # --- *** END PLACES SECTION *** ---\n",
    "\n",
    "\n",
    "\n",
    "# # --- *** CHECK RAHOON PLACE ID FOR PLOTTING *** ---\n",
    "#     rahoon_place_id = None # To store the 'P' index if Rahoon is found\n",
    "#     if 'galway_places_summary_df1' in locals() and isinstance(galway_places_summary_df1, pd.DataFrame) and not galway_places_summary_df1.empty:\n",
    "#         if 'place_name' in galway_places_summary_df1.columns:\n",
    "#             # Search for 'Rahoon' in the 'place_name' column \n",
    "#             rahoon_search_results = galway_places_summary_df1[galway_places_summary_df1['place_name'].str.contains('Rahoon', case=False, na=False)]\n",
    "\n",
    "#             if not rahoon_search_results.empty:\n",
    "#                 print(f\"\\n--- Found 'Rahoon' in galway_places_summary_df1 ---\")\n",
    "#                 rahoon_place_data = rahoon_search_results.iloc[0]\n",
    "#                 rahoon_place_id = rahoon_place_data.name \n",
    "#                 print(f\"Place Name: {rahoon_place_data['place_name']}\")\n",
    "#                 print(f\"Index (ID): {rahoon_place_id}\")\n",
    "#                 print(f\"Latitude: {rahoon_place_data['latitude']}\")\n",
    "#                 print(f\"Longitude: {rahoon_place_data['longitude']}\")\n",
    "#             else:\n",
    "#                 print(\"\\nPlace name containing 'Rahoon' not found in galway_places_summary_df1.\")\n",
    "#         else:\n",
    "#             print(\"\\n'place_name' column not found in galway_places_summary_df1.\")\n",
    "#     else:\n",
    "#         print(\"\\nDataFrame 'galway_places_summary_df1' not available for searching 'Rahoon'.\")\n",
    "\n",
    "\n",
    "# # --- *** CREATE BUILDINGS SUMMARY DATAFRAME *** ---\n",
    "#     print(\"\\nCreating DataFrame for Galway Buildings with Type and Coordinates...\")\n",
    "#     galway_buildings_summary_df = None # Initialize\n",
    "#     if 'buildings' in galway_gdfs and not galway_gdfs['buildings'].empty:\n",
    "#         buildings_data = []\n",
    "\n",
    "#         # Check what columns are available in the buildings layer\n",
    "#         print(f\"Available columns in buildings layer: {galway_gdfs['buildings'].columns.tolist()}\")\n",
    "\n",
    "#         # Extract building info - name, osm_id, and type (typically in fclass or type column)\n",
    "#         for idx, row in galway_gdfs['buildings'][galway_gdfs['buildings'].geometry.is_valid].iterrows():\n",
    "#             osm_id = row.get('osm_id', None)\n",
    "#             name = row.get('name', None)\n",
    "#             building_type = None\n",
    "#             for type_col in ['fclass', 'type', 'building']:\n",
    "#                 if type_col in row and row[type_col] is not None:\n",
    "#                     building_type = row[type_col]; break\n",
    "#             try:\n",
    "#                 centroid = row.geometry.centroid\n",
    "#                 if centroid and centroid.is_valid:\n",
    "#                     buildings_data.append({\n",
    "#                         'building_name': name, 'osm_id': osm_id, 'building_type': building_type,\n",
    "#                         'latitude': centroid.y, 'longitude': centroid.x\n",
    "#                     })\n",
    "#             except Exception as e: print(f\"Error calculating centroid for building {osm_id}: {e}\")\n",
    "\n",
    "#         if buildings_data:\n",
    "#             galway_buildings_summary_df = pd.DataFrame(buildings_data)\n",
    "#             print(f\"Created DataFrame 'galway_buildings_summary_df' with {len(galway_buildings_summary_df)} buildings.\")\n",
    "#             print(galway_buildings_summary_df.head())\n",
    "#         else: print(\"No valid building data found to create summary DataFrame.\")\n",
    "#     else: print(\"Clipped 'buildings' GeoDataFrame not found or is empty.\")\n",
    "\n",
    "#     # --- *** REFINE BUILDING SUMMARY DATAFRAME *** ---\n",
    "#     galway_buildings_summary_df1 = None # Initialize\n",
    "#     if galway_buildings_summary_df is not None:\n",
    "#         galway_buildings_summary_df1 = galway_buildings_summary_df[galway_buildings_summary_df['building_name'].notnull()].copy()\n",
    "#         galway_buildings_summary_df1 = galway_buildings_summary_df1.sort_values('building_name')\n",
    "#         building_indices = [f'B{i+1}' for i in range(len(galway_buildings_summary_df1))]\n",
    "#         galway_buildings_summary_df1.index = building_indices\n",
    "#         print(\"\\nCreated filtered DataFrame 'galway_buildings_summary_df1' with named buildings:\")\n",
    "#         print(f\"Number of named buildings: {len(galway_buildings_summary_df1)}\")\n",
    "#         print(\"\\nFirst few rows of filtered DataFrame:\")\n",
    "#         print(galway_buildings_summary_df1.head())\n",
    "#     else: print(\"Cannot create filtered DataFrame as galway_buildings_summary_df is None\")\n",
    "\n",
    "#     # --- *** END BUILDINGS SECTION *** ---\n",
    "\n",
    "\n",
    "\n",
    "#     # -- PRODUCTION CODE --\n",
    "#     # we should have place_nearby_stop_ids and building_nearby_stop_ids to run this code\n",
    "#     # Filter bus_stops_gdf for these specific stops\n",
    "#     # Ensure bus_stops_gdf is available and has a 'stop_id' column\n",
    "#     bus_stops_near_rahoon_gdf = None\n",
    "#     bus_stops_near_portershed_gdf = None\n",
    "\n",
    "#     if 'bus_stops_gdf' in locals() and bus_stops_gdf is not None and not bus_stops_gdf.empty:\n",
    "#         if place_nearby_stop_ids:\n",
    "#             bus_stops_near_rahoon_gdf = bus_stops_gdf[bus_stops_gdf['stop_id'].isin(place_nearby_stop_ids)]\n",
    "#         if building_nearby_stop_ids:\n",
    "#             bus_stops_near_portershed_gdf = bus_stops_gdf[bus_stops_gdf['stop_id'].isin(building_nearby_stop_ids)]\n",
    "#     else:\n",
    "#         print(\"Warning: bus_stops_gdf not available for filtering nearby stops.\")\n",
    "#     # --- END PRODUCTION CODE ---\n",
    "\n",
    "\n",
    "\n",
    "#     # --- *** PLOTTING CLIPPED GALWAY DATA *** ---\n",
    "#     print(\"\\nPlotting clipped Galway map layers...\")\n",
    "#     fig, ax = plt.subplots(figsize=(18, 18), facecolor='white', dpi=250)\n",
    "\n",
    "#     # Define base colors\n",
    "#     color_water = '#a8dff5'; color_land = '#f2f4f6'; color_parks = '#cceac4'\n",
    "#     color_buildings_osm = '#d8cabc' # Renamed to avoid conflict\n",
    "#     color_roads = '#aaaaaa'; color_rail = '#a0a0a0';color_place_text = '#36454F'  # Charcoal for place labels\n",
    "    \n",
    "#     # Define bus stop color\n",
    "#     color_bus_stops_blue = '#1E90FF' # Dodger blue for all bus stops\n",
    "\n",
    "\n",
    "#     # -- PRODUCTION CODE --\n",
    "#     # Define NEW colors for nearby stops\n",
    "#     color_nearby_rahoon_stops = '#32CD32'  # Lime Green\n",
    "#     color_nearby_portershed_stops = '#FFD700' # Gold (or choose another distinct color like a different shade of green)\n",
    "#     nearby_stop_marker_size = 35 # Slightly larger than general, smaller than POIs\n",
    "#     # --- END PRODUCTION CODE ---\n",
    "\n",
    "#     # Set background\n",
    "#     ax.set_facecolor(color_land)\n",
    "\n",
    "#     # Define approximate z-orders\n",
    "#     zorder_landuse=1; zorder_water_poly=2; zorder_parks=3; zorder_buildings_layer=4 # General buildings layer\n",
    "#     zorder_waterways=5; zorder_railways=6; zorder_roads=7;\n",
    "#     zorder_bus_stops_plot = 8    # Z-order for general bus stops\n",
    "#     zorder_nearby_stops_plot = zorder_bus_stops_plot + 0.1  # production code\n",
    "#     zorder_place_text = 9        # Z-order for general place name labels\n",
    "\n",
    "#     # Z-orders for the specific B422 building highlight - Portershed\n",
    "#     zorder_building_b422_point = 10  \n",
    "#     zorder_building_b422_text = 11  \n",
    "\n",
    "#     # Z-orders for the specific 'Rahoon' place highlight\n",
    "#     zorder_rahoon_place_point = 10 \n",
    "#     zorder_rahoon_place_text = 11  \n",
    "\n",
    "\n",
    "#     zorder_boundary = 12   # Boundary should be having highest zorder to frame everything\n",
    "    \n",
    "\n",
    "#     # Plot base layers\n",
    "#     if 'landuse' in galway_gdfs: galway_gdfs['landuse'].plot(ax=ax, column='fclass', categorical=True, cmap='Pastel2', alpha=0.4, zorder=zorder_landuse)\n",
    "#     if 'water_poly' in galway_gdfs: galway_gdfs['water_poly'].plot(ax=ax, color=color_water, edgecolor='none', zorder=zorder_water_poly)\n",
    "#     if 'landuse' in galway_gdfs and 'fclass' in galway_gdfs['landuse'].columns:\n",
    "#         parks_gdf = galway_gdfs['landuse'][galway_gdfs['landuse']['fclass'] == 'park']\n",
    "#         if not parks_gdf.empty: parks_gdf.plot(ax=ax, color=color_parks, edgecolor='none', zorder=zorder_parks)\n",
    "#     if 'buildings' in galway_gdfs: galway_gdfs['buildings'].plot(ax=ax, facecolor=color_buildings_osm, alpha=0.7, lw=0.5, edgecolor=color_buildings_osm, zorder=zorder_buildings_layer)\n",
    "#     if 'waterways' in galway_gdfs: galway_gdfs['waterways'].plot(ax=ax, color=color_water, linewidth=1.0, zorder=zorder_waterways)\n",
    "#     if 'railways' in galway_gdfs:\n",
    "#         galway_gdfs['railways'].plot(ax=ax, color='#ffffff', linewidth=2.0, linestyle='-', zorder=zorder_railways)\n",
    "#         galway_gdfs['railways'].plot(ax=ax, color=color_rail, linewidth=1.0, linestyle='-', zorder=zorder_railways + 0.1)\n",
    "#     if 'roads' in galway_gdfs: galway_gdfs['roads'].plot(ax=ax, color=color_roads, linewidth=0.8, zorder=zorder_roads)\n",
    "\n",
    "#     # --- Plot ALL Bus Stops from gstops_df_v1 as BLUE DOTS ---\n",
    "#     if bus_stops_gdf is not None and not bus_stops_gdf.empty:\n",
    "#         bus_stops_gdf.plot(\n",
    "#             ax=ax,\n",
    "#             color=color_bus_stops_blue, # Use the defined blue color\n",
    "#             marker='o',\n",
    "#             markersize=15,             \n",
    "#             edgecolor='black',        \n",
    "#             linewidth=0.5,\n",
    "#             alpha=0.9,\n",
    "#             zorder=zorder_bus_stops_plot, # Ensure they are on top of most layers\n",
    "#             label='Bus Stops (All)'\n",
    "#         )\n",
    "#         print(f\"Plotted {len(bus_stops_gdf)} bus stops from gstops_df_v1 as blue dots.\")\n",
    "#     else:\n",
    "#         print(\"No bus stops from gstops_df_v1 to plot.\")\n",
    "\n",
    "#     # -- PRODUCTION CODE --\n",
    "#     # --- *** NEW: Plot Bus Stops Near Rahoon *** ---\n",
    "#     if bus_stops_near_rahoon_gdf is not None and not bus_stops_near_rahoon_gdf.empty:\n",
    "#         bus_stops_near_rahoon_gdf.plot(\n",
    "#             ax=ax,\n",
    "#             color=color_nearby_rahoon_stops,\n",
    "#             marker='o',\n",
    "#             markersize=nearby_stop_marker_size, # Use new smaller size\n",
    "#             edgecolor='black',\n",
    "#             linewidth=0.7,\n",
    "#             alpha=0.9,\n",
    "#             zorder=zorder_nearby_stops_plot, # Higher z-order\n",
    "#             label='Stops near Rahoon'\n",
    "#         )\n",
    "#         print(f\"Plotted {len(bus_stops_near_rahoon_gdf)} bus stops near Rahoon.\")\n",
    "\n",
    "#             # --- *** NEW: Plot Bus Stops Near Portershed *** ---\n",
    "#     if bus_stops_near_portershed_gdf is not None and not bus_stops_near_portershed_gdf.empty:\n",
    "#         bus_stops_near_portershed_gdf.plot(\n",
    "#             ax=ax,\n",
    "#             color=color_nearby_portershed_stops,\n",
    "#             marker='o',\n",
    "#             markersize=nearby_stop_marker_size, # Use new smaller size\n",
    "#             edgecolor='black',\n",
    "#             linewidth=0.7,\n",
    "#             alpha=0.9,\n",
    "#             zorder=zorder_nearby_stops_plot, # Higher z-order\n",
    "#             label='Stops near Portershed'\n",
    "#         )\n",
    "#         print(f\"Plotted {len(bus_stops_near_portershed_gdf)} bus stops near Portershed.\")\n",
    "#     # --- END PRODUCTION CODE ---\n",
    "\n",
    "    \n",
    "\n",
    "#     # --- Plot Place Names (No Circles) ---\n",
    "#     if galway_places_summary_df is not None and not galway_places_summary_df.empty:\n",
    "#         print(f\"Plotting {len(galway_places_summary_df)} place names...\")\n",
    "#         plotted_place_names_map = set()\n",
    "#         for idx, row in galway_places_summary_df.iterrows():\n",
    "#             label = row['place_name']; point_x = row['longitude']; point_y = row['latitude']\n",
    "#             if label not in plotted_place_names_map:\n",
    "#                 ax.text(point_x, point_y + 0.0002, label, fontsize=8, color=color_place_text,\n",
    "#                         ha='center', va='bottom', zorder=zorder_place_text, fontweight='normal',\n",
    "#                         path_effects=[matplotlib.patheffects.withStroke(linewidth=1, foreground='w')])\n",
    "#                 plotted_place_names_map.add(label)\n",
    "#         print(\"Place names plotted.\")\n",
    "\n",
    "#     # --- *** PLOT B422 BUILDING - PORTERSHED *** ---\n",
    "#     if 'galway_buildings_summary_df1' in locals() and galway_buildings_summary_df1 is not None and not galway_buildings_summary_df1.empty:\n",
    "#         building_point_color = '#FF5733' # Orange\n",
    "#         building_text_color = '#000000'  # Black\n",
    "#         plotted_b422 = False\n",
    "#         # Ensure B422 exists in your dataframe's index\n",
    "#         if 'B422' in galway_buildings_summary_df1.index:\n",
    "#             row = galway_buildings_summary_df1.loc['B422']\n",
    "#             point_x = row['longitude']\n",
    "#             point_y = row['latitude']\n",
    "#             building_name = row['building_name']\n",
    "            \n",
    "#             # Plot orange circle for B422\n",
    "#             plt.scatter(point_x, point_y, s=60, color=building_point_color, edgecolor='black', # Increased size (s=60)\n",
    "#                         linewidth=1, alpha=0.9, zorder=zorder_building_b422_point, label=f'Building: {building_name}')\n",
    "            \n",
    "#             # Plot name label for B422\n",
    "#             ax.text(point_x, point_y + 0.0003, building_name, fontsize=7, color=building_text_color, \n",
    "#                     ha='center', va='bottom', zorder=zorder_building_b422_text, fontweight='bold',\n",
    "#                     path_effects=[matplotlib.patheffects.withStroke(linewidth=1, foreground='white')])\n",
    "#             plotted_b422 = True\n",
    "#             print(f\"Plotted orange circle and name label for building B422 ('{building_name}').\")\n",
    "#         else:\n",
    "#             print(\"Building B422 not found in the DataFrame 'galway_buildings_summary_df1'.\")\n",
    "#     else:\n",
    "#         print(\"DataFrame 'galway_buildings_summary_df1' not available for plotting B422.\")\n",
    "#     # --- *** END OF B422 PLOTTING CODE *** ---   \n",
    "\n",
    "\n",
    "\n",
    "#     # --- *** PLOT SPECIFIC PLACE 'RAHOON' *** ---\n",
    "#     if 'rahoon_place_id' in locals() and rahoon_place_id is not None and \\\n",
    "#        'galway_places_summary_df1' in locals() and galway_places_summary_df1 is not None and \\\n",
    "#        not galway_places_summary_df1.empty:\n",
    "\n",
    "#         if rahoon_place_id in galway_places_summary_df1.index:\n",
    "#             place_row = galway_places_summary_df1.loc[rahoon_place_id]\n",
    "#             point_x = place_row['longitude']\n",
    "#             point_y = place_row['latitude']\n",
    "#             place_name_label = place_row['place_name'] \n",
    "\n",
    "#             place_point_color = '#9400D3' # Dark Violet \n",
    "#             place_text_color = '#000000'   # Black\n",
    "\n",
    "#             # Plot distinct circle for 'Rahoon'\n",
    "#             plt.scatter(point_x, point_y, s=70, color=place_point_color, edgecolor='black', \n",
    "#                         linewidth=1, alpha=0.9, zorder=zorder_rahoon_place_point, label=f'Place: {place_name_label}')\n",
    "\n",
    "#             # Plot name label for 'Rahoon'\n",
    "#             ax.text(point_x, point_y + 0.00035, place_name_label, fontsize=7.5, color=place_text_color,\n",
    "#                     ha='center', va='bottom', zorder=zorder_rahoon_place_text, fontweight='bold',\n",
    "#                     path_effects=[matplotlib.patheffects.withStroke(linewidth=1, foreground='white')])\n",
    "#             print(f\"Plotted distinct circle and name label for place: '{place_name_label}' (ID: {rahoon_place_id}).\")\n",
    "#         else:\n",
    "#             print(f\"Place with ID '{rahoon_place_id}' (expected to be Rahoon) not found in galway_places_summary_df1.index for plotting.\")\n",
    "#     else:\n",
    "#         print(\"Rahoon was not identified or 'galway_places_summary_df1' is not available for plotting specific place.\")\n",
    "#     # --- *** END OF 'RAHOON' PLOTTING CODE *** ---\n",
    "\n",
    "\n",
    "#     # Plot boundary outline for context last\n",
    "#     boundary_gdf.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.5, linestyle='--', zorder=zorder_boundary)\n",
    "\n",
    "#     # --- Set Map Bounds ---\n",
    "#     if 'roads' in galway_gdfs and not galway_gdfs['roads'].empty:\n",
    "#         minx, miny, maxx, maxy = galway_gdfs['roads'].total_bounds\n",
    "#     else:\n",
    "#         minx, miny, maxx, maxy = boundary_gdf.total_bounds\n",
    "#     margin_factor = 0.02\n",
    "#     margin_x = (maxx - minx) * margin_factor\n",
    "#     margin_y = (maxy - miny) * margin_factor\n",
    "#     ax.set_xlim(minx - margin_x, maxx + margin_x)\n",
    "#     ax.set_ylim(miny - margin_y, maxy + margin_y)\n",
    "#     ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "#     # (Inside your main plotting cell [12], within the `try` block, in the\n",
    "# #  \"PLOT THE SHORTEST JOURNEY PATH\" section)\n",
    "\n",
    "# # --- *** PLOT THE SHORTEST JOURNEY PATH *** ---\n",
    "#     if shortest_journey_path_details:\n",
    "#         print(\"\\nPlotting the shortest journey path...\")\n",
    "        \n",
    "#         # Define styles for the path\n",
    "#         walk_color = 'dimgrey' # Darker grey \n",
    "#         walk_linestyle = '--'\n",
    "#         walk_linewidth = 1.8\n",
    "        \n",
    "  \n",
    "#         transit_color = '#E60000' \n",
    "#         transit_linestyle = '-'\n",
    "#         transit_linewidth = 3.0\n",
    "#         transit_alpha = 0.85 # transparency\n",
    "\n",
    "#         path_zorder = zorder_boundary + 1 \n",
    "\n",
    "#         # Helper to get coordinates \n",
    "#         def get_node_coords(node_id, graph, places_gdf, buildings_gdf, stops_gdf):\n",
    "    \n",
    "#             # return (longitude, latitude) for any given node ID\n",
    "#             if graph.has_node(node_id):\n",
    "#                 node_data = graph.nodes[node_id]\n",
    "#                 if 'x' in node_data and 'y' in node_data:\n",
    "#                     return node_data['x'], node_data['y']\n",
    "                \n",
    "#                 # Fallback logic using 'type' and 'id' if present in G.nodes[node_id]\n",
    "#                 node_type = node_data.get('type')\n",
    "#                 original_id = node_data.get('id', node_id) # \n",
    "\n",
    "#                 if node_type == 'place' and places_gdf is not None:\n",
    "#                     # places_gdf index is the ID we need (e.g. 'P1', 'P2')\n",
    "#                     if original_id in places_gdf.index:\n",
    "#                         geom = places_gdf.loc[original_id].geometry\n",
    "#                         return geom.x, geom.y\n",
    "#                 elif node_type == 'building' and buildings_gdf is not None:\n",
    "#                     # buildings_gdf index is the ID (e.g. 'B1', 'B22')\n",
    "#                     if original_id in buildings_gdf.index:\n",
    "#                         geom = buildings_gdf.loc[original_id].geometry\n",
    "#                         return geom.x, geom.y\n",
    "#                 elif node_type == 'bus_stop' and stops_gdf is not None and 'stop_id' in stops_gdf.columns:\n",
    "#                     # 'original_id' value to match in 'stop_id' column\n",
    "#                     stop_row = stops_gdf[stops_gdf['stop_id'] == original_id]\n",
    "#                     if not stop_row.empty:\n",
    "#                         geom = stop_row.iloc[0].geometry\n",
    "#                         return geom.x, geom.y\n",
    "#             return None, None\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             # 1. Origin POI to Origin Bus Stop (Walk)\n",
    "#             o_poi_x, o_poi_y = get_node_coords(shortest_journey_path_details['origin_poi_node_id'], G, galway_places_summary_df1, galway_buildings_summary_df1, bus_stops_gdf)\n",
    "#             o_bs_x, o_bs_y = get_node_coords(shortest_journey_path_details['origin_bus_stop_id'], G, galway_places_summary_df1, galway_buildings_summary_df1, bus_stops_gdf)\n",
    "#             walk_path_label_set = False\n",
    "#             if o_poi_x and o_bs_x:\n",
    "#                 ax.plot([o_poi_x, o_bs_x], [o_poi_y, o_bs_y], color=walk_color, linestyle=walk_linestyle, \n",
    "#                         linewidth=walk_linewidth, zorder=path_zorder, label='Shortest Path (Walk)', alpha=transit_alpha)\n",
    "#                 walk_path_label_set = True\n",
    "        \n",
    "\n",
    "#             # 2. Transit Segment (Bus)\n",
    "#             transit_nodes_sequence = shortest_journey_path_details['transit_stop_sequence_ids']\n",
    "#             if len(transit_nodes_sequence) >= 2:\n",
    "#                 transit_path_label_set = False\n",
    "#                 for i in range(len(transit_nodes_sequence) - 1):\n",
    "#                     from_node_id = transit_nodes_sequence[i]\n",
    "#                     to_node_id = transit_nodes_sequence[i+1]\n",
    "#                     from_x, from_y = get_node_coords(from_node_id, G, galway_places_summary_df1, galway_buildings_summary_df1, bus_stops_gdf)\n",
    "#                     to_x, to_y = get_node_coords(to_node_id, G, galway_places_summary_df1, galway_buildings_summary_df1, bus_stops_gdf)\n",
    "                    \n",
    "#                     if from_x and to_x:\n",
    "#                         current_label = 'Shortest Path (Transit)' if not transit_path_label_set else None\n",
    "#                         line, = ax.plot([from_x, to_x], [from_y, to_y], color=transit_color, linestyle=transit_linestyle, \n",
    "#                                     linewidth=transit_linewidth, zorder=path_zorder, label=current_label, alpha=transit_alpha)\n",
    "#                         # Add path effect\n",
    "#                         line.set_path_effects([path_effects.Stroke(linewidth=transit_linewidth + 1.5, foreground='white', alpha=0.6),\n",
    "#                                             path_effects.Normal()])\n",
    "#                         if not transit_path_label_set: transit_path_label_set = True\n",
    "                \n",
    "\n",
    "#             # 3. Destination Bus Stop to Destination POI (Walk)\n",
    "#             d_bs_x, d_bs_y = get_node_coords(shortest_journey_path_details['destination_bus_stop_id'], G, galway_places_summary_df1, galway_buildings_summary_df1, bus_stops_gdf)\n",
    "#             d_poi_x, d_poi_y = get_node_coords(shortest_journey_path_details['destination_poi_node_id'], G, galway_places_summary_df1, galway_buildings_summary_df1, bus_stops_gdf)\n",
    "#             if d_bs_x and d_poi_x:\n",
    "#                 current_walk_label = 'Shortest Path (Walk)' if not walk_path_label_set else None\n",
    "#                 ax.plot([d_bs_x, d_poi_x], [d_bs_y, d_poi_y], color=walk_color, linestyle=walk_linestyle, \n",
    "#                         linewidth=walk_linewidth, zorder=path_zorder, label=current_walk_label, alpha=transit_alpha)\n",
    "                \n",
    "                \n",
    "#             # Update legend to ensure new labels are included and no duplicates\n",
    "#             handles, labels = ax.get_legend_handles_labels()\n",
    "#             by_label = dict(zip(labels, handles)) \n",
    "#             ax.legend(by_label.values(), by_label.keys(), loc='upper right', fontsize='small')\n",
    "#             print(\"Shortest journey path plotted (if details were available).\")\n",
    "\n",
    "#         except Exception as e_plot:\n",
    "#             print(f\"Error during shortest path plotting: {e_plot}\")\n",
    "#             # import traceback # Already imported at top level of notebook usually\n",
    "#             traceback.print_exc()\n",
    "#     else:\n",
    "#         print(\"\\nNo shortest journey path details available to plot.\")\n",
    "\n",
    "\n",
    "#     # Final plot adjustments\n",
    "#     ax.set_title(f\"Galway Map with Bus Stops (from gstops_df_v1)\", color='black', fontsize=16)\n",
    "#     plt.legend(loc='upper right') # add a legend\n",
    "#     plt.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    " \n",
    "\n",
    "# except FileNotFoundError as e:\n",
    "#     print(f\"\\n--- File Error ---\\n{e}\\nPlease ensure file paths are correct.\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"\\n--- Import Error Occurred ---\\nError: {e}\\nPlease ensure required libraries are installed.\")\n",
    "# except ValueError as e:\n",
    "#     print(f\"\\n--- Value Error ---\\n{e}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\n--- An Unexpected Error Occurred ---\\nError: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3_10_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
